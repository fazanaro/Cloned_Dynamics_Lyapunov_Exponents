//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-19324607
// Cuda compilation tools, release 7.0, V7.0.27
// Based on LLVM 3.4svn
//

.version 4.2
.target sm_20
.address_size 64

	// .globl	_Z20calculateGlobalIndexv
.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd
(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
;
.const .align 8 .b8 __cudart_i2opi_d[144] = {8, 93, 141, 31, 177, 95, 251, 107, 234, 146, 82, 138, 247, 57, 7, 61, 123, 241, 229, 235, 199, 186, 39, 117, 45, 234, 95, 158, 102, 63, 70, 79, 183, 9, 203, 39, 207, 126, 54, 109, 31, 109, 10, 90, 139, 17, 47, 239, 15, 152, 5, 222, 255, 151, 248, 31, 59, 40, 249, 189, 139, 95, 132, 156, 244, 57, 83, 131, 57, 214, 145, 57, 65, 126, 95, 180, 38, 112, 156, 233, 132, 68, 187, 46, 245, 53, 130, 232, 62, 167, 41, 177, 28, 235, 29, 254, 28, 146, 209, 9, 234, 46, 73, 6, 224, 210, 77, 66, 58, 110, 36, 183, 97, 197, 187, 222, 171, 99, 81, 254, 65, 144, 67, 60, 153, 149, 98, 219, 192, 221, 52, 245, 209, 87, 39, 252, 41, 21, 68, 78, 110, 131, 249, 162};
.const .align 8 .b8 __cudart_sin_cos_coeffs[128] = {186, 94, 120, 249, 101, 219, 229, 61, 70, 210, 176, 44, 241, 229, 90, 190, 146, 227, 172, 105, 227, 29, 199, 62, 161, 98, 219, 25, 160, 1, 42, 191, 24, 8, 17, 17, 17, 17, 129, 63, 84, 85, 85, 85, 85, 85, 197, 191, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 100, 129, 253, 32, 131, 255, 168, 189, 40, 133, 239, 193, 167, 238, 33, 62, 217, 230, 6, 142, 79, 126, 146, 190, 233, 188, 221, 25, 160, 1, 250, 62, 71, 93, 193, 22, 108, 193, 86, 191, 81, 85, 85, 85, 85, 85, 165, 63, 0, 0, 0, 0, 0, 0, 224, 191, 0, 0, 0, 0, 0, 0, 240, 63};

.visible .func  (.param .b64 func_retval0) _Z20calculateGlobalIndexv(

)
{
	.reg .s32 	%r<11>;
	.reg .s64 	%rd<4>;


	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ctaid.y;
	mov.u32 	%r3, %nctaid.x;
	mad.lo.s32 	%r4, %r3, %r2, %r1;
	mov.u32 	%r5, %tid.x;
	mov.u32 	%r6, %ntid.x;
	mov.u32 	%r7, %tid.y;
	mad.lo.s32 	%r8, %r7, %r6, %r5;
	cvt.u64.u32	%rd1, %r8;
	mov.u32 	%r9, %ntid.y;
	mul.lo.s32 	%r10, %r9, %r6;
	mul.wide.u32 	%rd2, %r10, %r4;
	add.s64 	%rd3, %rd2, %rd1;
	st.param.b64	[func_retval0+0], %rd3;
	ret;
}

	// .globl	_Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j
.visible .entry _Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j(
	.param .u64 _Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_0,
	.param .u64 _Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_1,
	.param .u64 _Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_2,
	.param .u64 _Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_3,
	.param .u64 _Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_4,
	.param .u64 _Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_5,
	.param .u64 _Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_6,
	.param .u64 _Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_7,
	.param .u64 _Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_8,
	.param .u64 _Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_9,
	.param .u64 _Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_10,
	.param .u64 _Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_11,
	.param .u64 _Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_12,
	.param .u64 _Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_13,
	.param .u64 _Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_14,
	.param .u64 _Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_15,
	.param .u64 _Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_16,
	.param .u64 _Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_17,
	.param .u64 _Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_18,
	.param .u32 _Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_19
)
{
	.local .align 4 .b8 	__local_depot1[4];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<218>;
	.reg .s32 	%r<411>;
	.reg .f64 	%fd<1474>;
	.reg .s64 	%rd<187>;


	mov.u64 	%rd186, __local_depot1;
	cvta.local.u64 	%SP, %rd186;
	ld.param.u64 	%rd5, [_Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_3];
	ld.param.u64 	%rd6, [_Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_4];
	ld.param.u64 	%rd7, [_Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_5];
	ld.param.u64 	%rd8, [_Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_6];
	ld.param.u64 	%rd9, [_Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_7];
	ld.param.u64 	%rd10, [_Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_8];
	ld.param.u64 	%rd11, [_Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_9];
	ld.param.u64 	%rd12, [_Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_10];
	ld.param.u64 	%rd13, [_Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_11];
	ld.param.u64 	%rd14, [_Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_12];
	ld.param.u64 	%rd15, [_Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_13];
	ld.param.u64 	%rd16, [_Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_14];
	ld.param.u64 	%rd17, [_Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_15];
	ld.param.u64 	%rd18, [_Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_16];
	ld.param.u64 	%rd19, [_Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_17];
	ld.param.u64 	%rd20, [_Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_18];
	mov.u32 	%r155, %nctaid.x;
	mov.u32 	%r156, %ctaid.y;
	mov.u32 	%r157, %ctaid.x;
	mad.lo.s32 	%r158, %r155, %r156, %r157;
	mov.u32 	%r159, %tid.y;
	mov.u32 	%r160, %ntid.x;
	mov.u32 	%r161, %tid.x;
	mad.lo.s32 	%r162, %r159, %r160, %r161;
	cvt.u64.u32	%rd21, %r162;
	mov.u32 	%r163, %ntid.y;
	mul.lo.s32 	%r164, %r163, %r160;
	mul.wide.u32 	%rd22, %r164, %r158;
	add.s64 	%rd1, %rd22, %rd21;
	ld.param.u32 	%rd23, [_Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_19];
	setp.ge.u64	%p1, %rd1, %rd23;
	@%p1 bra 	BB1_215;

	cvta.to.global.u64 	%rd24, %rd19;
	shl.b64 	%rd25, %rd1, 3;
	add.s64 	%rd26, %rd24, %rd25;
	ld.global.f64 	%fd1, [%rd26];
	cvta.to.global.u64 	%rd27, %rd18;
	add.s64 	%rd28, %rd27, %rd25;
	ld.global.f64 	%fd2, [%rd28];
	cvta.to.global.u64 	%rd29, %rd20;
	add.s64 	%rd30, %rd29, %rd25;
	ld.global.f64 	%fd3, [%rd30];
	cvta.to.global.u64 	%rd31, %rd17;
	add.s64 	%rd32, %rd31, %rd25;
	ld.global.f64 	%fd4, [%rd32];
	mov.f64 	%fd566, 0d3FE0000000000000;
	div.rn.f64 	%fd567, %fd566, %fd4;
	cvt.rzi.s32.f64	%r1, %fd567;
	cvta.to.global.u64 	%rd33, %rd5;
	add.s64 	%rd34, %rd33, %rd25;
	ld.global.f64 	%fd1369, [%rd34];
	cvta.to.global.u64 	%rd35, %rd6;
	add.s64 	%rd36, %rd35, %rd25;
	ld.global.f64 	%fd1358, [%rd36];
	cvta.to.global.u64 	%rd37, %rd7;
	add.s64 	%rd38, %rd37, %rd25;
	ld.global.f64 	%fd1349, [%rd38];
	cvta.to.global.u64 	%rd39, %rd8;
	add.s64 	%rd40, %rd39, %rd25;
	ld.global.f64 	%fd1337, [%rd40];
	cvta.to.global.u64 	%rd41, %rd9;
	add.s64 	%rd42, %rd41, %rd25;
	ld.global.f64 	%fd1327, [%rd42];
	cvta.to.global.u64 	%rd43, %rd10;
	add.s64 	%rd44, %rd43, %rd25;
	ld.global.f64 	%fd1317, [%rd44];
	cvta.to.global.u64 	%rd45, %rd11;
	add.s64 	%rd46, %rd45, %rd25;
	ld.global.f64 	%fd1307, [%rd46];
	cvta.to.global.u64 	%rd47, %rd12;
	add.s64 	%rd48, %rd47, %rd25;
	ld.global.f64 	%fd1299, [%rd48];
	cvta.to.global.u64 	%rd49, %rd13;
	add.s64 	%rd50, %rd49, %rd25;
	ld.global.f64 	%fd1291, [%rd50];
	cvta.to.global.u64 	%rd51, %rd14;
	add.s64 	%rd52, %rd51, %rd25;
	ld.global.f64 	%fd1283, [%rd52];
	cvta.to.global.u64 	%rd53, %rd15;
	add.s64 	%rd54, %rd53, %rd25;
	ld.global.f64 	%fd1275, [%rd54];
	cvta.to.global.u64 	%rd55, %rd16;
	add.s64 	%rd56, %rd55, %rd25;
	ld.global.f64 	%fd1267, [%rd56];
	mul.f64 	%fd17, %fd4, 0d3FE0000000000000;
	fma.rn.f64 	%fd568, %fd4, 0d4000000000000000, %fd4;
	fma.rn.f64 	%fd569, %fd4, 0d4000000000000000, %fd568;
	add.f64 	%fd570, %fd4, %fd569;
	mul.f64 	%fd18, %fd570, 0d3FC5555555555555;
	mov.f64 	%fd1340, 0d0000000000000000;
	mov.f64 	%fd1339, %fd1340;
	mov.f64 	%fd1338, %fd1340;
	mov.u32 	%r333, 0;

BB1_2:
	mov.f64 	%fd1361, %fd1368;
	mov.f64 	%fd1366, %fd1369;
	mov.f64 	%fd1367, %fd1361;
	mov.f64 	%fd1351, %fd1357;
	mov.f64 	%fd1355, %fd1358;
	mov.f64 	%fd1356, %fd1351;
	mov.f64 	%fd1342, %fd1348;
	mov.f64 	%fd1346, %fd1349;
	mov.f64 	%fd1347, %fd1342;
	mov.f64 	%fd1328, %fd1337;
	mov.f64 	%fd1334, %fd1333;
	mov.f64 	%fd1336, %fd1328;
	mov.f64 	%fd1318, %fd1327;
	mov.f64 	%fd1324, %fd1323;
	mov.f64 	%fd1326, %fd1318;
	mov.f64 	%fd1308, %fd1317;
	mov.f64 	%fd1314, %fd1313;
	mov.f64 	%fd1316, %fd1308;
	mov.f64 	%fd1300, %fd1307;
	mov.f64 	%fd1305, %fd1304;
	mov.f64 	%fd1306, %fd1300;
	mov.f64 	%fd1292, %fd1299;
	mov.f64 	%fd1297, %fd1296;
	mov.f64 	%fd1298, %fd1292;
	mov.f64 	%fd1284, %fd1291;
	mov.f64 	%fd1289, %fd1288;
	mov.f64 	%fd1290, %fd1284;
	mov.f64 	%fd1276, %fd1283;
	mov.f64 	%fd1281, %fd1280;
	mov.f64 	%fd1282, %fd1276;
	mov.f64 	%fd1268, %fd1275;
	mov.f64 	%fd1273, %fd1272;
	mov.f64 	%fd1274, %fd1268;
	mov.f64 	%fd1260, %fd1267;
	mov.f64 	%fd1265, %fd1264;
	mov.f64 	%fd1266, %fd1260;
	setp.eq.s32	%p2, %r1, 0;
	@%p2 bra 	BB1_165;

	mov.u32 	%r168, 3;
	abs.s32 	%r169, %r168;
	shr.u32 	%r5, %r169, 1;
	mov.u32 	%r334, 0;

BB1_4:
	mov.f64 	%fd1360, %fd1366;
	mov.f64 	%fd57, %fd1360;
	mov.f64 	%fd1330, %fd1336;
	mov.f64 	%fd54, %fd1330;
	mov.f64 	%fd1320, %fd1326;
	mov.f64 	%fd53, %fd1320;
	mov.f64 	%fd1310, %fd1316;
	mov.f64 	%fd52, %fd1310;
	mov.u32 	%r332, 3;
	abs.s32 	%r331, %r332;
	and.b32  	%r330, %r331, 1;
	mul.f64 	%fd58, %fd4, %fd1355;
	setp.eq.s32	%p3, %r330, 0;
	selp.f64	%fd1370, 0d3FF0000000000000, %fd57, %p3;
	setp.eq.s32	%p4, %r5, 0;
	mov.f64 	%fd1365, %fd57;
	mov.u32 	%r380, %r5;
	@%p4 bra 	BB1_6;

BB1_5:
	mov.u32 	%r7, %r380;
	mov.f64 	%fd61, %fd1365;
	and.b32  	%r170, %r7, 1;
	setp.eq.b32	%p5, %r170, 1;
	not.pred 	%p6, %p5;
	mul.f64 	%fd62, %fd61, %fd61;
	mul.f64 	%fd571, %fd1370, %fd62;
	selp.f64	%fd1370, %fd1370, %fd571, %p6;
	shr.u32 	%r8, %r7, 1;
	setp.ne.s32	%p7, %r8, 0;
	mov.f64 	%fd1365, %fd62;
	mov.u32 	%r380, %r8;
	@%p7 bra 	BB1_5;

BB1_6:
	mul.f64 	%fd1371, %fd3, %fd1346;
	abs.f64 	%fd572, %fd1371;
	setp.neu.f64	%p8, %fd572, 0d7FF0000000000000;
	@%p8 bra 	BB1_8;

	mov.f64 	%fd573, 0d0000000000000000;
	mul.rn.f64 	%fd1371, %fd1371, %fd573;

BB1_8:
	mul.f64 	%fd574, %fd1371, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r335, %fd574;
	add.u64 	%rd57, %SP, 0;
	cvta.to.local.u64 	%rd58, %rd57;
	st.local.u32 	[%rd58], %r335;
	cvt.rn.f64.s32	%fd575, %r335;
	neg.f64 	%fd576, %fd575;
	mov.f64 	%fd577, 0d3FF921FB54442D18;
	fma.rn.f64 	%fd578, %fd576, %fd577, %fd1371;
	mov.f64 	%fd579, 0d3C91A62633145C00;
	fma.rn.f64 	%fd580, %fd576, %fd579, %fd578;
	mov.f64 	%fd581, 0d397B839A252049C0;
	fma.rn.f64 	%fd1372, %fd576, %fd581, %fd580;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r171}, %fd1371;
	}
	and.b32  	%r172, %r171, 2145386496;
	setp.lt.u32	%p9, %r172, 1105199104;
	@%p9 bra 	BB1_10;

	// Callseq Start 0
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1371;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd57;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1372, [retval0+0];
	
	//{
	}// Callseq End 0
	ld.local.u32 	%r335, [%rd58];

BB1_10:
	add.s32 	%r12, %r335, 1;
	and.b32  	%r173, %r12, 1;
	shl.b32 	%r174, %r173, 3;
	setp.eq.s32	%p10, %r173, 0;
	selp.f64	%fd582, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p10;
	mul.wide.u32 	%rd61, %r174, 8;
	mov.u64 	%rd62, __cudart_sin_cos_coeffs;
	add.s64 	%rd63, %rd61, %rd62;
	ld.const.f64 	%fd583, [%rd63+8];
	mul.rn.f64 	%fd71, %fd1372, %fd1372;
	fma.rn.f64 	%fd584, %fd582, %fd71, %fd583;
	ld.const.f64 	%fd585, [%rd63+16];
	fma.rn.f64 	%fd586, %fd584, %fd71, %fd585;
	ld.const.f64 	%fd587, [%rd63+24];
	fma.rn.f64 	%fd588, %fd586, %fd71, %fd587;
	ld.const.f64 	%fd589, [%rd63+32];
	fma.rn.f64 	%fd590, %fd588, %fd71, %fd589;
	ld.const.f64 	%fd591, [%rd63+40];
	fma.rn.f64 	%fd592, %fd590, %fd71, %fd591;
	ld.const.f64 	%fd593, [%rd63+48];
	fma.rn.f64 	%fd72, %fd592, %fd71, %fd593;
	fma.rn.f64 	%fd1373, %fd72, %fd1372, %fd1372;
	@%p10 bra 	BB1_12;

	mov.f64 	%fd594, 0d3FF0000000000000;
	fma.rn.f64 	%fd1373, %fd72, %fd71, %fd594;

BB1_12:
	and.b32  	%r175, %r12, 2;
	setp.eq.s32	%p11, %r175, 0;
	@%p11 bra 	BB1_14;

	mov.f64 	%fd595, 0d0000000000000000;
	mov.f64 	%fd596, 0dBFF0000000000000;
	fma.rn.f64 	%fd1373, %fd1373, %fd596, %fd595;

BB1_14:
	mul.f64 	%fd597, %fd1, %fd1355;
	sub.f64 	%fd598, %fd57, %fd1370;
	sub.f64 	%fd599, %fd598, %fd597;
	fma.rn.f64 	%fd600, %fd2, %fd1373, %fd599;
	mul.f64 	%fd78, %fd4, %fd600;
	fma.rn.f64 	%fd79, %fd78, 0d3FE0000000000000, %fd1355;
	mul.f64 	%fd80, %fd4, %fd79;
	fma.rn.f64 	%fd81, %fd58, 0d3FE0000000000000, %fd57;
	selp.f64	%fd1375, 0d3FF0000000000000, %fd81, %p3;
	mov.f64 	%fd1374, %fd81;
	mov.u32 	%r379, %r5;
	@%p4 bra 	BB1_16;

BB1_15:
	mov.f64 	%fd84, %fd1374;
	and.b32  	%r176, %r379, 1;
	setp.eq.b32	%p14, %r176, 1;
	not.pred 	%p15, %p14;
	mul.f64 	%fd85, %fd84, %fd84;
	mul.f64 	%fd601, %fd1375, %fd85;
	selp.f64	%fd1375, %fd1375, %fd601, %p15;
	shr.u32 	%r379, %r379, 1;
	setp.ne.s32	%p16, %r379, 0;
	mov.f64 	%fd1374, %fd85;
	@%p16 bra 	BB1_15;

BB1_16:
	mul.f64 	%fd602, %fd1, %fd79;
	sub.f64 	%fd603, %fd81, %fd1375;
	sub.f64 	%fd88, %fd603, %fd602;
	add.f64 	%fd604, %fd17, %fd1346;
	mul.f64 	%fd89, %fd3, %fd604;
	abs.f64 	%fd90, %fd89;
	setp.neu.f64	%p17, %fd90, 0d7FF0000000000000;
	mov.f64 	%fd1382, %fd89;
	@%p17 bra 	BB1_18;

	mov.f64 	%fd605, 0d0000000000000000;
	mul.rn.f64 	%fd91, %fd89, %fd605;
	mov.f64 	%fd1382, %fd91;

BB1_18:
	mov.f64 	%fd92, %fd1382;
	mul.f64 	%fd606, %fd92, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r336, %fd606;
	st.local.u32 	[%rd58], %r336;
	cvt.rn.f64.s32	%fd607, %r336;
	neg.f64 	%fd608, %fd607;
	fma.rn.f64 	%fd610, %fd608, %fd577, %fd92;
	fma.rn.f64 	%fd612, %fd608, %fd579, %fd610;
	fma.rn.f64 	%fd1376, %fd608, %fd581, %fd612;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r177}, %fd92;
	}
	and.b32  	%r178, %r177, 2145386496;
	setp.lt.u32	%p18, %r178, 1105199104;
	@%p18 bra 	BB1_20;

	// Callseq Start 1
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd92;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd57;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1376, [retval0+0];
	
	//{
	}// Callseq End 1
	ld.local.u32 	%r336, [%rd58];

BB1_20:
	add.s32 	%r18, %r336, 1;
	and.b32  	%r179, %r18, 1;
	shl.b32 	%r180, %r179, 3;
	setp.eq.s32	%p19, %r179, 0;
	selp.f64	%fd614, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p19;
	mul.wide.u32 	%rd68, %r180, 8;
	add.s64 	%rd70, %rd68, %rd62;
	ld.const.f64 	%fd615, [%rd70+8];
	mul.rn.f64 	%fd96, %fd1376, %fd1376;
	fma.rn.f64 	%fd616, %fd614, %fd96, %fd615;
	ld.const.f64 	%fd617, [%rd70+16];
	fma.rn.f64 	%fd618, %fd616, %fd96, %fd617;
	ld.const.f64 	%fd619, [%rd70+24];
	fma.rn.f64 	%fd620, %fd618, %fd96, %fd619;
	ld.const.f64 	%fd621, [%rd70+32];
	fma.rn.f64 	%fd622, %fd620, %fd96, %fd621;
	ld.const.f64 	%fd623, [%rd70+40];
	fma.rn.f64 	%fd624, %fd622, %fd96, %fd623;
	ld.const.f64 	%fd625, [%rd70+48];
	fma.rn.f64 	%fd97, %fd624, %fd96, %fd625;
	fma.rn.f64 	%fd1377, %fd97, %fd1376, %fd1376;
	@%p19 bra 	BB1_22;

	mov.f64 	%fd626, 0d3FF0000000000000;
	fma.rn.f64 	%fd1377, %fd97, %fd96, %fd626;

BB1_22:
	and.b32  	%r181, %r18, 2;
	setp.eq.s32	%p20, %r181, 0;
	@%p20 bra 	BB1_24;

	mov.f64 	%fd627, 0d0000000000000000;
	mov.f64 	%fd628, 0dBFF0000000000000;
	fma.rn.f64 	%fd1377, %fd1377, %fd628, %fd627;

BB1_24:
	fma.rn.f64 	%fd629, %fd2, %fd1377, %fd88;
	mul.f64 	%fd103, %fd4, %fd629;
	fma.rn.f64 	%fd104, %fd103, 0d3FE0000000000000, %fd1355;
	mul.f64 	%fd105, %fd4, %fd104;
	fma.rn.f64 	%fd106, %fd80, 0d3FE0000000000000, %fd57;
	selp.f64	%fd1379, 0d3FF0000000000000, %fd106, %p3;
	mov.f64 	%fd1378, %fd106;
	mov.u32 	%r378, %r5;
	@%p4 bra 	BB1_26;

BB1_25:
	mov.f64 	%fd109, %fd1378;
	and.b32  	%r182, %r378, 1;
	setp.eq.b32	%p23, %r182, 1;
	not.pred 	%p24, %p23;
	mul.f64 	%fd110, %fd109, %fd109;
	mul.f64 	%fd630, %fd1379, %fd110;
	selp.f64	%fd1379, %fd1379, %fd630, %p24;
	shr.u32 	%r378, %r378, 1;
	setp.ne.s32	%p25, %r378, 0;
	mov.f64 	%fd1378, %fd110;
	@%p25 bra 	BB1_25;

BB1_26:
	mul.f64 	%fd631, %fd1, %fd104;
	sub.f64 	%fd632, %fd106, %fd1379;
	sub.f64 	%fd113, %fd632, %fd631;
	mov.f64 	%fd1381, %fd89;
	@%p17 bra 	BB1_28;

	mov.f64 	%fd633, 0d0000000000000000;
	mul.rn.f64 	%fd1381, %fd89, %fd633;

BB1_28:
	mul.f64 	%fd634, %fd1381, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r337, %fd634;
	st.local.u32 	[%rd58], %r337;
	cvt.rn.f64.s32	%fd635, %r337;
	neg.f64 	%fd636, %fd635;
	fma.rn.f64 	%fd638, %fd636, %fd577, %fd1381;
	fma.rn.f64 	%fd640, %fd636, %fd579, %fd638;
	fma.rn.f64 	%fd1383, %fd636, %fd581, %fd640;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r183}, %fd1381;
	}
	and.b32  	%r184, %r183, 2145386496;
	setp.lt.u32	%p27, %r184, 1105199104;
	@%p27 bra 	BB1_30;

	// Callseq Start 2
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1381;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd57;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1383, [retval0+0];
	
	//{
	}// Callseq End 2
	ld.local.u32 	%r337, [%rd58];

BB1_30:
	add.s32 	%r24, %r337, 1;
	and.b32  	%r185, %r24, 1;
	shl.b32 	%r186, %r185, 3;
	setp.eq.s32	%p28, %r185, 0;
	selp.f64	%fd642, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p28;
	mul.wide.u32 	%rd75, %r186, 8;
	add.s64 	%rd77, %rd75, %rd62;
	ld.const.f64 	%fd643, [%rd77+8];
	mul.rn.f64 	%fd119, %fd1383, %fd1383;
	fma.rn.f64 	%fd644, %fd642, %fd119, %fd643;
	ld.const.f64 	%fd645, [%rd77+16];
	fma.rn.f64 	%fd646, %fd644, %fd119, %fd645;
	ld.const.f64 	%fd647, [%rd77+24];
	fma.rn.f64 	%fd648, %fd646, %fd119, %fd647;
	ld.const.f64 	%fd649, [%rd77+32];
	fma.rn.f64 	%fd650, %fd648, %fd119, %fd649;
	ld.const.f64 	%fd651, [%rd77+40];
	fma.rn.f64 	%fd652, %fd650, %fd119, %fd651;
	ld.const.f64 	%fd653, [%rd77+48];
	fma.rn.f64 	%fd120, %fd652, %fd119, %fd653;
	fma.rn.f64 	%fd1384, %fd120, %fd1383, %fd1383;
	@%p28 bra 	BB1_32;

	mov.f64 	%fd654, 0d3FF0000000000000;
	fma.rn.f64 	%fd1384, %fd120, %fd119, %fd654;

BB1_32:
	and.b32  	%r187, %r24, 2;
	setp.eq.s32	%p29, %r187, 0;
	@%p29 bra 	BB1_34;

	mov.f64 	%fd655, 0d0000000000000000;
	mov.f64 	%fd656, 0dBFF0000000000000;
	fma.rn.f64 	%fd1384, %fd1384, %fd656, %fd655;

BB1_34:
	fma.rn.f64 	%fd657, %fd2, %fd1384, %fd113;
	mul.f64 	%fd126, %fd4, %fd657;
	add.f64 	%fd127, %fd57, %fd105;
	selp.f64	%fd1386, 0d3FF0000000000000, %fd127, %p3;
	mov.f64 	%fd1385, %fd127;
	mov.u32 	%r377, %r5;
	@%p4 bra 	BB1_36;

BB1_35:
	mov.f64 	%fd130, %fd1385;
	and.b32  	%r188, %r377, 1;
	setp.eq.b32	%p32, %r188, 1;
	not.pred 	%p33, %p32;
	mul.f64 	%fd131, %fd130, %fd130;
	mul.f64 	%fd658, %fd1386, %fd131;
	selp.f64	%fd1386, %fd1386, %fd658, %p33;
	shr.u32 	%r377, %r377, 1;
	setp.ne.s32	%p34, %r377, 0;
	mov.f64 	%fd1385, %fd131;
	@%p34 bra 	BB1_35;

BB1_36:
	add.f64 	%fd659, %fd4, %fd1346;
	mul.f64 	%fd1387, %fd3, %fd659;
	abs.f64 	%fd660, %fd1387;
	setp.neu.f64	%p35, %fd660, 0d7FF0000000000000;
	@%p35 bra 	BB1_38;

	mov.f64 	%fd661, 0d0000000000000000;
	mul.rn.f64 	%fd1387, %fd1387, %fd661;

BB1_38:
	mul.f64 	%fd662, %fd1387, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r338, %fd662;
	st.local.u32 	[%rd58], %r338;
	cvt.rn.f64.s32	%fd663, %r338;
	neg.f64 	%fd664, %fd663;
	fma.rn.f64 	%fd666, %fd664, %fd577, %fd1387;
	fma.rn.f64 	%fd668, %fd664, %fd579, %fd666;
	fma.rn.f64 	%fd1388, %fd664, %fd581, %fd668;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r189}, %fd1387;
	}
	and.b32  	%r190, %r189, 2145386496;
	setp.lt.u32	%p36, %r190, 1105199104;
	@%p36 bra 	BB1_40;

	// Callseq Start 3
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1387;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd57;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1388, [retval0+0];
	
	//{
	}// Callseq End 3
	ld.local.u32 	%r338, [%rd58];

BB1_40:
	add.s32 	%r30, %r338, 1;
	and.b32  	%r191, %r30, 1;
	shl.b32 	%r192, %r191, 3;
	setp.eq.s32	%p37, %r191, 0;
	selp.f64	%fd670, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p37;
	mul.wide.u32 	%rd82, %r192, 8;
	add.s64 	%rd84, %rd82, %rd62;
	ld.const.f64 	%fd671, [%rd84+8];
	mul.rn.f64 	%fd140, %fd1388, %fd1388;
	fma.rn.f64 	%fd672, %fd670, %fd140, %fd671;
	ld.const.f64 	%fd673, [%rd84+16];
	fma.rn.f64 	%fd674, %fd672, %fd140, %fd673;
	ld.const.f64 	%fd675, [%rd84+24];
	fma.rn.f64 	%fd676, %fd674, %fd140, %fd675;
	ld.const.f64 	%fd677, [%rd84+32];
	fma.rn.f64 	%fd678, %fd676, %fd140, %fd677;
	ld.const.f64 	%fd679, [%rd84+40];
	fma.rn.f64 	%fd680, %fd678, %fd140, %fd679;
	ld.const.f64 	%fd681, [%rd84+48];
	fma.rn.f64 	%fd141, %fd680, %fd140, %fd681;
	fma.rn.f64 	%fd1389, %fd141, %fd1388, %fd1388;
	@%p37 bra 	BB1_42;

	mov.f64 	%fd682, 0d3FF0000000000000;
	fma.rn.f64 	%fd1389, %fd141, %fd140, %fd682;

BB1_42:
	and.b32  	%r193, %r30, 2;
	setp.eq.s32	%p38, %r193, 0;
	@%p38 bra 	BB1_44;

	mov.f64 	%fd683, 0d0000000000000000;
	mov.f64 	%fd684, 0dBFF0000000000000;
	fma.rn.f64 	%fd1389, %fd1389, %fd684, %fd683;

BB1_44:
	add.f64 	%fd685, %fd1355, %fd126;
	mul.f64 	%fd147, %fd4, %fd685;
	mul.f64 	%fd686, %fd1, %fd685;
	sub.f64 	%fd687, %fd127, %fd1386;
	sub.f64 	%fd688, %fd687, %fd686;
	fma.rn.f64 	%fd148, %fd2, %fd1389, %fd688;
	mul.f64 	%fd149, %fd4, %fd1306;
	mul.f64 	%fd150, %fd4, %fd1298;
	mul.f64 	%fd151, %fd4, %fd1290;
	selp.f64	%fd1390, 0d3FF0000000000000, %fd54, %p3;
	mov.f64 	%fd1335, %fd54;
	mov.u32 	%r376, %r5;
	@%p4 bra 	BB1_46;

BB1_45:
	mov.f64 	%fd154, %fd1335;
	and.b32  	%r194, %r376, 1;
	setp.eq.b32	%p41, %r194, 1;
	not.pred 	%p42, %p41;
	mul.f64 	%fd155, %fd154, %fd154;
	mul.f64 	%fd689, %fd1390, %fd155;
	selp.f64	%fd1390, %fd1390, %fd689, %p42;
	shr.u32 	%r376, %r376, 1;
	setp.ne.s32	%p43, %r376, 0;
	mov.f64 	%fd1335, %fd155;
	@%p43 bra 	BB1_45;

BB1_46:
	mul.f64 	%fd1391, %fd3, %fd1282;
	abs.f64 	%fd690, %fd1391;
	setp.neu.f64	%p44, %fd690, 0d7FF0000000000000;
	@%p44 bra 	BB1_48;

	mov.f64 	%fd691, 0d0000000000000000;
	mul.rn.f64 	%fd1391, %fd1391, %fd691;

BB1_48:
	mul.f64 	%fd692, %fd1391, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r339, %fd692;
	st.local.u32 	[%rd58], %r339;
	cvt.rn.f64.s32	%fd693, %r339;
	neg.f64 	%fd694, %fd693;
	fma.rn.f64 	%fd696, %fd694, %fd577, %fd1391;
	fma.rn.f64 	%fd698, %fd694, %fd579, %fd696;
	fma.rn.f64 	%fd1392, %fd694, %fd581, %fd698;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r195}, %fd1391;
	}
	and.b32  	%r196, %r195, 2145386496;
	setp.lt.u32	%p45, %r196, 1105199104;
	@%p45 bra 	BB1_50;

	// Callseq Start 4
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1391;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd57;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1392, [retval0+0];
	
	//{
	}// Callseq End 4
	ld.local.u32 	%r339, [%rd58];

BB1_50:
	add.s32 	%r36, %r339, 1;
	and.b32  	%r197, %r36, 1;
	shl.b32 	%r198, %r197, 3;
	setp.eq.s32	%p46, %r197, 0;
	selp.f64	%fd700, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p46;
	mul.wide.u32 	%rd89, %r198, 8;
	add.s64 	%rd91, %rd89, %rd62;
	ld.const.f64 	%fd701, [%rd91+8];
	mul.rn.f64 	%fd164, %fd1392, %fd1392;
	fma.rn.f64 	%fd702, %fd700, %fd164, %fd701;
	ld.const.f64 	%fd703, [%rd91+16];
	fma.rn.f64 	%fd704, %fd702, %fd164, %fd703;
	ld.const.f64 	%fd705, [%rd91+24];
	fma.rn.f64 	%fd706, %fd704, %fd164, %fd705;
	ld.const.f64 	%fd707, [%rd91+32];
	fma.rn.f64 	%fd708, %fd706, %fd164, %fd707;
	ld.const.f64 	%fd709, [%rd91+40];
	fma.rn.f64 	%fd710, %fd708, %fd164, %fd709;
	ld.const.f64 	%fd711, [%rd91+48];
	fma.rn.f64 	%fd165, %fd710, %fd164, %fd711;
	fma.rn.f64 	%fd1393, %fd165, %fd1392, %fd1392;
	@%p46 bra 	BB1_52;

	mov.f64 	%fd712, 0d3FF0000000000000;
	fma.rn.f64 	%fd1393, %fd165, %fd164, %fd712;

BB1_52:
	and.b32  	%r199, %r36, 2;
	setp.eq.s32	%p47, %r199, 0;
	@%p47 bra 	BB1_54;

	mov.f64 	%fd713, 0d0000000000000000;
	mov.f64 	%fd714, 0dBFF0000000000000;
	fma.rn.f64 	%fd1393, %fd1393, %fd714, %fd713;

BB1_54:
	mul.f64 	%fd715, %fd1, %fd1306;
	sub.f64 	%fd716, %fd54, %fd1390;
	sub.f64 	%fd717, %fd716, %fd715;
	fma.rn.f64 	%fd718, %fd2, %fd1393, %fd717;
	mul.f64 	%fd171, %fd4, %fd718;
	selp.f64	%fd1394, 0d3FF0000000000000, %fd53, %p3;
	mov.f64 	%fd1325, %fd53;
	mov.u32 	%r375, %r5;
	@%p4 bra 	BB1_56;

BB1_55:
	mov.f64 	%fd174, %fd1325;
	and.b32  	%r200, %r375, 1;
	setp.eq.b32	%p50, %r200, 1;
	not.pred 	%p51, %p50;
	mul.f64 	%fd175, %fd174, %fd174;
	mul.f64 	%fd719, %fd1394, %fd175;
	selp.f64	%fd1394, %fd1394, %fd719, %p51;
	shr.u32 	%r375, %r375, 1;
	setp.ne.s32	%p52, %r375, 0;
	mov.f64 	%fd1325, %fd175;
	@%p52 bra 	BB1_55;

BB1_56:
	mul.f64 	%fd1395, %fd3, %fd1274;
	abs.f64 	%fd720, %fd1395;
	setp.neu.f64	%p53, %fd720, 0d7FF0000000000000;
	@%p53 bra 	BB1_58;

	mov.f64 	%fd721, 0d0000000000000000;
	mul.rn.f64 	%fd1395, %fd1395, %fd721;

BB1_58:
	mul.f64 	%fd722, %fd1395, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r340, %fd722;
	st.local.u32 	[%rd58], %r340;
	cvt.rn.f64.s32	%fd723, %r340;
	neg.f64 	%fd724, %fd723;
	fma.rn.f64 	%fd726, %fd724, %fd577, %fd1395;
	fma.rn.f64 	%fd728, %fd724, %fd579, %fd726;
	fma.rn.f64 	%fd1396, %fd724, %fd581, %fd728;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r201}, %fd1395;
	}
	and.b32  	%r202, %r201, 2145386496;
	setp.lt.u32	%p54, %r202, 1105199104;
	@%p54 bra 	BB1_60;

	// Callseq Start 5
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1395;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd57;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1396, [retval0+0];
	
	//{
	}// Callseq End 5
	ld.local.u32 	%r340, [%rd58];

BB1_60:
	add.s32 	%r42, %r340, 1;
	and.b32  	%r203, %r42, 1;
	shl.b32 	%r204, %r203, 3;
	setp.eq.s32	%p55, %r203, 0;
	selp.f64	%fd730, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p55;
	mul.wide.u32 	%rd96, %r204, 8;
	add.s64 	%rd98, %rd96, %rd62;
	ld.const.f64 	%fd731, [%rd98+8];
	mul.rn.f64 	%fd184, %fd1396, %fd1396;
	fma.rn.f64 	%fd732, %fd730, %fd184, %fd731;
	ld.const.f64 	%fd733, [%rd98+16];
	fma.rn.f64 	%fd734, %fd732, %fd184, %fd733;
	ld.const.f64 	%fd735, [%rd98+24];
	fma.rn.f64 	%fd736, %fd734, %fd184, %fd735;
	ld.const.f64 	%fd737, [%rd98+32];
	fma.rn.f64 	%fd738, %fd736, %fd184, %fd737;
	ld.const.f64 	%fd739, [%rd98+40];
	fma.rn.f64 	%fd740, %fd738, %fd184, %fd739;
	ld.const.f64 	%fd741, [%rd98+48];
	fma.rn.f64 	%fd185, %fd740, %fd184, %fd741;
	fma.rn.f64 	%fd1397, %fd185, %fd1396, %fd1396;
	@%p55 bra 	BB1_62;

	mov.f64 	%fd742, 0d3FF0000000000000;
	fma.rn.f64 	%fd1397, %fd185, %fd184, %fd742;

BB1_62:
	and.b32  	%r205, %r42, 2;
	setp.eq.s32	%p56, %r205, 0;
	@%p56 bra 	BB1_64;

	mov.f64 	%fd743, 0d0000000000000000;
	mov.f64 	%fd744, 0dBFF0000000000000;
	fma.rn.f64 	%fd1397, %fd1397, %fd744, %fd743;

BB1_64:
	mul.f64 	%fd745, %fd1, %fd1298;
	sub.f64 	%fd746, %fd53, %fd1394;
	sub.f64 	%fd747, %fd746, %fd745;
	fma.rn.f64 	%fd748, %fd2, %fd1397, %fd747;
	mul.f64 	%fd191, %fd4, %fd748;
	selp.f64	%fd1398, 0d3FF0000000000000, %fd52, %p3;
	mov.f64 	%fd1315, %fd52;
	mov.u32 	%r374, %r5;
	@%p4 bra 	BB1_66;

BB1_65:
	mov.f64 	%fd194, %fd1315;
	and.b32  	%r206, %r374, 1;
	setp.eq.b32	%p59, %r206, 1;
	not.pred 	%p60, %p59;
	mul.f64 	%fd195, %fd194, %fd194;
	mul.f64 	%fd749, %fd1398, %fd195;
	selp.f64	%fd1398, %fd1398, %fd749, %p60;
	shr.u32 	%r374, %r374, 1;
	setp.ne.s32	%p61, %r374, 0;
	mov.f64 	%fd1315, %fd195;
	@%p61 bra 	BB1_65;

BB1_66:
	mul.f64 	%fd1399, %fd3, %fd1266;
	abs.f64 	%fd750, %fd1399;
	setp.neu.f64	%p62, %fd750, 0d7FF0000000000000;
	@%p62 bra 	BB1_68;

	mov.f64 	%fd751, 0d0000000000000000;
	mul.rn.f64 	%fd1399, %fd1399, %fd751;

BB1_68:
	mul.f64 	%fd752, %fd1399, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r341, %fd752;
	st.local.u32 	[%rd58], %r341;
	cvt.rn.f64.s32	%fd753, %r341;
	neg.f64 	%fd754, %fd753;
	fma.rn.f64 	%fd756, %fd754, %fd577, %fd1399;
	fma.rn.f64 	%fd758, %fd754, %fd579, %fd756;
	fma.rn.f64 	%fd1400, %fd754, %fd581, %fd758;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r207}, %fd1399;
	}
	and.b32  	%r208, %r207, 2145386496;
	setp.lt.u32	%p63, %r208, 1105199104;
	@%p63 bra 	BB1_70;

	// Callseq Start 6
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1399;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd57;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1400, [retval0+0];
	
	//{
	}// Callseq End 6
	ld.local.u32 	%r341, [%rd58];

BB1_70:
	add.s32 	%r48, %r341, 1;
	and.b32  	%r209, %r48, 1;
	shl.b32 	%r210, %r209, 3;
	setp.eq.s32	%p64, %r209, 0;
	selp.f64	%fd760, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p64;
	mul.wide.u32 	%rd103, %r210, 8;
	add.s64 	%rd105, %rd103, %rd62;
	ld.const.f64 	%fd761, [%rd105+8];
	mul.rn.f64 	%fd204, %fd1400, %fd1400;
	fma.rn.f64 	%fd762, %fd760, %fd204, %fd761;
	ld.const.f64 	%fd763, [%rd105+16];
	fma.rn.f64 	%fd764, %fd762, %fd204, %fd763;
	ld.const.f64 	%fd765, [%rd105+24];
	fma.rn.f64 	%fd766, %fd764, %fd204, %fd765;
	ld.const.f64 	%fd767, [%rd105+32];
	fma.rn.f64 	%fd768, %fd766, %fd204, %fd767;
	ld.const.f64 	%fd769, [%rd105+40];
	fma.rn.f64 	%fd770, %fd768, %fd204, %fd769;
	ld.const.f64 	%fd771, [%rd105+48];
	fma.rn.f64 	%fd205, %fd770, %fd204, %fd771;
	fma.rn.f64 	%fd1401, %fd205, %fd1400, %fd1400;
	@%p64 bra 	BB1_72;

	mov.f64 	%fd772, 0d3FF0000000000000;
	fma.rn.f64 	%fd1401, %fd205, %fd204, %fd772;

BB1_72:
	and.b32  	%r211, %r48, 2;
	setp.eq.s32	%p65, %r211, 0;
	@%p65 bra 	BB1_74;

	mov.f64 	%fd773, 0d0000000000000000;
	mov.f64 	%fd774, 0dBFF0000000000000;
	fma.rn.f64 	%fd1401, %fd1401, %fd774, %fd773;

BB1_74:
	mul.f64 	%fd775, %fd1, %fd1290;
	sub.f64 	%fd776, %fd52, %fd1398;
	sub.f64 	%fd777, %fd776, %fd775;
	fma.rn.f64 	%fd778, %fd2, %fd1401, %fd777;
	mul.f64 	%fd211, %fd4, %fd778;
	fma.rn.f64 	%fd212, %fd171, 0d3FE0000000000000, %fd1306;
	mul.f64 	%fd213, %fd4, %fd212;
	fma.rn.f64 	%fd214, %fd191, 0d3FE0000000000000, %fd1298;
	mul.f64 	%fd215, %fd4, %fd214;
	fma.rn.f64 	%fd216, %fd211, 0d3FE0000000000000, %fd1290;
	mul.f64 	%fd217, %fd4, %fd216;
	fma.rn.f64 	%fd218, %fd149, 0d3FE0000000000000, %fd54;
	selp.f64	%fd1403, 0d3FF0000000000000, %fd218, %p3;
	mov.f64 	%fd1402, %fd218;
	mov.u32 	%r373, %r5;
	@%p4 bra 	BB1_76;

BB1_75:
	mov.f64 	%fd221, %fd1402;
	and.b32  	%r212, %r373, 1;
	setp.eq.b32	%p68, %r212, 1;
	not.pred 	%p69, %p68;
	mul.f64 	%fd222, %fd221, %fd221;
	mul.f64 	%fd779, %fd1403, %fd222;
	selp.f64	%fd1403, %fd1403, %fd779, %p69;
	shr.u32 	%r373, %r373, 1;
	setp.ne.s32	%p70, %r373, 0;
	mov.f64 	%fd1402, %fd222;
	@%p70 bra 	BB1_75;

BB1_76:
	mul.f64 	%fd225, %fd4, %fd148;
	mul.f64 	%fd780, %fd1, %fd212;
	sub.f64 	%fd781, %fd218, %fd1403;
	sub.f64 	%fd226, %fd781, %fd780;
	add.f64 	%fd782, %fd17, %fd1282;
	mul.f64 	%fd227, %fd3, %fd782;
	abs.f64 	%fd228, %fd227;
	setp.neu.f64	%p71, %fd228, 0d7FF0000000000000;
	mov.f64 	%fd1418, %fd227;
	@%p71 bra 	BB1_78;

	mov.f64 	%fd783, 0d0000000000000000;
	mul.rn.f64 	%fd229, %fd227, %fd783;
	mov.f64 	%fd1418, %fd229;

BB1_78:
	mov.f64 	%fd230, %fd1418;
	mul.f64 	%fd784, %fd230, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r342, %fd784;
	st.local.u32 	[%rd58], %r342;
	cvt.rn.f64.s32	%fd785, %r342;
	neg.f64 	%fd786, %fd785;
	fma.rn.f64 	%fd788, %fd786, %fd577, %fd230;
	fma.rn.f64 	%fd790, %fd786, %fd579, %fd788;
	fma.rn.f64 	%fd1404, %fd786, %fd581, %fd790;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r213}, %fd230;
	}
	and.b32  	%r214, %r213, 2145386496;
	setp.lt.u32	%p72, %r214, 1105199104;
	@%p72 bra 	BB1_80;

	// Callseq Start 7
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd230;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd57;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1404, [retval0+0];
	
	//{
	}// Callseq End 7
	ld.local.u32 	%r342, [%rd58];

BB1_80:
	add.s32 	%r54, %r342, 1;
	and.b32  	%r215, %r54, 1;
	shl.b32 	%r216, %r215, 3;
	setp.eq.s32	%p73, %r215, 0;
	selp.f64	%fd792, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p73;
	mul.wide.u32 	%rd110, %r216, 8;
	add.s64 	%rd112, %rd110, %rd62;
	ld.const.f64 	%fd793, [%rd112+8];
	mul.rn.f64 	%fd234, %fd1404, %fd1404;
	fma.rn.f64 	%fd794, %fd792, %fd234, %fd793;
	ld.const.f64 	%fd795, [%rd112+16];
	fma.rn.f64 	%fd796, %fd794, %fd234, %fd795;
	ld.const.f64 	%fd797, [%rd112+24];
	fma.rn.f64 	%fd798, %fd796, %fd234, %fd797;
	ld.const.f64 	%fd799, [%rd112+32];
	fma.rn.f64 	%fd800, %fd798, %fd234, %fd799;
	ld.const.f64 	%fd801, [%rd112+40];
	fma.rn.f64 	%fd802, %fd800, %fd234, %fd801;
	ld.const.f64 	%fd803, [%rd112+48];
	fma.rn.f64 	%fd235, %fd802, %fd234, %fd803;
	fma.rn.f64 	%fd1405, %fd235, %fd1404, %fd1404;
	@%p73 bra 	BB1_82;

	mov.f64 	%fd804, 0d3FF0000000000000;
	fma.rn.f64 	%fd1405, %fd235, %fd234, %fd804;

BB1_82:
	and.b32  	%r217, %r54, 2;
	setp.eq.s32	%p74, %r217, 0;
	@%p74 bra 	BB1_84;

	mov.f64 	%fd805, 0d0000000000000000;
	mov.f64 	%fd806, 0dBFF0000000000000;
	fma.rn.f64 	%fd1405, %fd1405, %fd806, %fd805;

BB1_84:
	fma.rn.f64 	%fd807, %fd2, %fd1405, %fd226;
	mul.f64 	%fd241, %fd4, %fd807;
	fma.rn.f64 	%fd242, %fd150, 0d3FE0000000000000, %fd53;
	selp.f64	%fd1407, 0d3FF0000000000000, %fd242, %p3;
	mov.f64 	%fd1406, %fd242;
	mov.u32 	%r372, %r5;
	@%p4 bra 	BB1_86;

BB1_85:
	mov.f64 	%fd245, %fd1406;
	and.b32  	%r218, %r372, 1;
	setp.eq.b32	%p77, %r218, 1;
	not.pred 	%p78, %p77;
	mul.f64 	%fd246, %fd245, %fd245;
	mul.f64 	%fd808, %fd1407, %fd246;
	selp.f64	%fd1407, %fd1407, %fd808, %p78;
	shr.u32 	%r372, %r372, 1;
	setp.ne.s32	%p79, %r372, 0;
	mov.f64 	%fd1406, %fd246;
	@%p79 bra 	BB1_85;

BB1_86:
	mul.f64 	%fd809, %fd1, %fd214;
	sub.f64 	%fd810, %fd242, %fd1407;
	sub.f64 	%fd249, %fd810, %fd809;
	add.f64 	%fd811, %fd17, %fd1274;
	mul.f64 	%fd250, %fd3, %fd811;
	abs.f64 	%fd251, %fd250;
	setp.neu.f64	%p80, %fd251, 0d7FF0000000000000;
	mov.f64 	%fd1425, %fd250;
	@%p80 bra 	BB1_88;

	mov.f64 	%fd812, 0d0000000000000000;
	mul.rn.f64 	%fd252, %fd250, %fd812;
	mov.f64 	%fd1425, %fd252;

BB1_88:
	mov.f64 	%fd253, %fd1425;
	mul.f64 	%fd813, %fd253, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r343, %fd813;
	st.local.u32 	[%rd58], %r343;
	cvt.rn.f64.s32	%fd814, %r343;
	neg.f64 	%fd815, %fd814;
	fma.rn.f64 	%fd817, %fd815, %fd577, %fd253;
	fma.rn.f64 	%fd819, %fd815, %fd579, %fd817;
	fma.rn.f64 	%fd1408, %fd815, %fd581, %fd819;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r219}, %fd253;
	}
	and.b32  	%r220, %r219, 2145386496;
	setp.lt.u32	%p81, %r220, 1105199104;
	@%p81 bra 	BB1_90;

	// Callseq Start 8
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd253;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd57;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1408, [retval0+0];
	
	//{
	}// Callseq End 8
	ld.local.u32 	%r343, [%rd58];

BB1_90:
	add.s32 	%r60, %r343, 1;
	and.b32  	%r221, %r60, 1;
	shl.b32 	%r222, %r221, 3;
	setp.eq.s32	%p82, %r221, 0;
	selp.f64	%fd821, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p82;
	mul.wide.u32 	%rd117, %r222, 8;
	add.s64 	%rd119, %rd117, %rd62;
	ld.const.f64 	%fd822, [%rd119+8];
	mul.rn.f64 	%fd257, %fd1408, %fd1408;
	fma.rn.f64 	%fd823, %fd821, %fd257, %fd822;
	ld.const.f64 	%fd824, [%rd119+16];
	fma.rn.f64 	%fd825, %fd823, %fd257, %fd824;
	ld.const.f64 	%fd826, [%rd119+24];
	fma.rn.f64 	%fd827, %fd825, %fd257, %fd826;
	ld.const.f64 	%fd828, [%rd119+32];
	fma.rn.f64 	%fd829, %fd827, %fd257, %fd828;
	ld.const.f64 	%fd830, [%rd119+40];
	fma.rn.f64 	%fd831, %fd829, %fd257, %fd830;
	ld.const.f64 	%fd832, [%rd119+48];
	fma.rn.f64 	%fd258, %fd831, %fd257, %fd832;
	fma.rn.f64 	%fd1409, %fd258, %fd1408, %fd1408;
	@%p82 bra 	BB1_92;

	mov.f64 	%fd833, 0d3FF0000000000000;
	fma.rn.f64 	%fd1409, %fd258, %fd257, %fd833;

BB1_92:
	and.b32  	%r223, %r60, 2;
	setp.eq.s32	%p83, %r223, 0;
	@%p83 bra 	BB1_94;

	mov.f64 	%fd834, 0d0000000000000000;
	mov.f64 	%fd835, 0dBFF0000000000000;
	fma.rn.f64 	%fd1409, %fd1409, %fd835, %fd834;

BB1_94:
	fma.rn.f64 	%fd836, %fd2, %fd1409, %fd249;
	mul.f64 	%fd264, %fd4, %fd836;
	fma.rn.f64 	%fd265, %fd151, 0d3FE0000000000000, %fd52;
	selp.f64	%fd1411, 0d3FF0000000000000, %fd265, %p3;
	mov.f64 	%fd1410, %fd265;
	mov.u32 	%r371, %r5;
	@%p4 bra 	BB1_96;

BB1_95:
	mov.f64 	%fd268, %fd1410;
	and.b32  	%r224, %r371, 1;
	setp.eq.b32	%p86, %r224, 1;
	not.pred 	%p87, %p86;
	mul.f64 	%fd269, %fd268, %fd268;
	mul.f64 	%fd837, %fd1411, %fd269;
	selp.f64	%fd1411, %fd1411, %fd837, %p87;
	shr.u32 	%r371, %r371, 1;
	setp.ne.s32	%p88, %r371, 0;
	mov.f64 	%fd1410, %fd269;
	@%p88 bra 	BB1_95;

BB1_96:
	mul.f64 	%fd838, %fd1, %fd216;
	sub.f64 	%fd839, %fd265, %fd1411;
	sub.f64 	%fd272, %fd839, %fd838;
	add.f64 	%fd840, %fd17, %fd1266;
	mul.f64 	%fd273, %fd3, %fd840;
	abs.f64 	%fd274, %fd273;
	setp.neu.f64	%p89, %fd274, 0d7FF0000000000000;
	mov.f64 	%fd1432, %fd273;
	@%p89 bra 	BB1_98;

	mov.f64 	%fd841, 0d0000000000000000;
	mul.rn.f64 	%fd275, %fd273, %fd841;
	mov.f64 	%fd1432, %fd275;

BB1_98:
	mov.f64 	%fd276, %fd1432;
	mul.f64 	%fd842, %fd276, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r344, %fd842;
	st.local.u32 	[%rd58], %r344;
	cvt.rn.f64.s32	%fd843, %r344;
	neg.f64 	%fd844, %fd843;
	fma.rn.f64 	%fd846, %fd844, %fd577, %fd276;
	fma.rn.f64 	%fd848, %fd844, %fd579, %fd846;
	fma.rn.f64 	%fd1412, %fd844, %fd581, %fd848;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r225}, %fd276;
	}
	and.b32  	%r226, %r225, 2145386496;
	setp.lt.u32	%p90, %r226, 1105199104;
	@%p90 bra 	BB1_100;

	// Callseq Start 9
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd276;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd57;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1412, [retval0+0];
	
	//{
	}// Callseq End 9
	ld.local.u32 	%r344, [%rd58];

BB1_100:
	add.s32 	%r66, %r344, 1;
	and.b32  	%r227, %r66, 1;
	shl.b32 	%r228, %r227, 3;
	setp.eq.s32	%p91, %r227, 0;
	selp.f64	%fd850, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p91;
	mul.wide.u32 	%rd124, %r228, 8;
	add.s64 	%rd126, %rd124, %rd62;
	ld.const.f64 	%fd851, [%rd126+8];
	mul.rn.f64 	%fd280, %fd1412, %fd1412;
	fma.rn.f64 	%fd852, %fd850, %fd280, %fd851;
	ld.const.f64 	%fd853, [%rd126+16];
	fma.rn.f64 	%fd854, %fd852, %fd280, %fd853;
	ld.const.f64 	%fd855, [%rd126+24];
	fma.rn.f64 	%fd856, %fd854, %fd280, %fd855;
	ld.const.f64 	%fd857, [%rd126+32];
	fma.rn.f64 	%fd858, %fd856, %fd280, %fd857;
	ld.const.f64 	%fd859, [%rd126+40];
	fma.rn.f64 	%fd860, %fd858, %fd280, %fd859;
	ld.const.f64 	%fd861, [%rd126+48];
	fma.rn.f64 	%fd281, %fd860, %fd280, %fd861;
	fma.rn.f64 	%fd1413, %fd281, %fd1412, %fd1412;
	@%p91 bra 	BB1_102;

	mov.f64 	%fd862, 0d3FF0000000000000;
	fma.rn.f64 	%fd1413, %fd281, %fd280, %fd862;

BB1_102:
	and.b32  	%r229, %r66, 2;
	setp.eq.s32	%p92, %r229, 0;
	@%p92 bra 	BB1_104;

	mov.f64 	%fd863, 0d0000000000000000;
	mov.f64 	%fd864, 0dBFF0000000000000;
	fma.rn.f64 	%fd1413, %fd1413, %fd864, %fd863;

BB1_104:
	fma.rn.f64 	%fd865, %fd2, %fd1413, %fd272;
	mul.f64 	%fd287, %fd4, %fd865;
	fma.rn.f64 	%fd288, %fd241, 0d3FE0000000000000, %fd1306;
	mul.f64 	%fd289, %fd4, %fd288;
	fma.rn.f64 	%fd290, %fd264, 0d3FE0000000000000, %fd1298;
	mul.f64 	%fd291, %fd4, %fd290;
	fma.rn.f64 	%fd292, %fd287, 0d3FE0000000000000, %fd1290;
	mul.f64 	%fd293, %fd4, %fd292;
	fma.rn.f64 	%fd294, %fd213, 0d3FE0000000000000, %fd54;
	selp.f64	%fd1415, 0d3FF0000000000000, %fd294, %p3;
	mov.f64 	%fd1414, %fd294;
	mov.u32 	%r370, %r5;
	@%p4 bra 	BB1_106;

BB1_105:
	mov.f64 	%fd297, %fd1414;
	and.b32  	%r230, %r370, 1;
	setp.eq.b32	%p95, %r230, 1;
	not.pred 	%p96, %p95;
	mul.f64 	%fd298, %fd297, %fd297;
	mul.f64 	%fd866, %fd1415, %fd298;
	selp.f64	%fd1415, %fd1415, %fd866, %p96;
	shr.u32 	%r370, %r370, 1;
	setp.ne.s32	%p97, %r370, 0;
	mov.f64 	%fd1414, %fd298;
	@%p97 bra 	BB1_105;

BB1_106:
	mul.f64 	%fd867, %fd1, %fd288;
	sub.f64 	%fd868, %fd294, %fd1415;
	sub.f64 	%fd301, %fd868, %fd867;
	mov.f64 	%fd1417, %fd227;
	@%p71 bra 	BB1_108;

	mov.f64 	%fd869, 0d0000000000000000;
	mul.rn.f64 	%fd1417, %fd227, %fd869;

BB1_108:
	mul.f64 	%fd870, %fd1417, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r345, %fd870;
	st.local.u32 	[%rd58], %r345;
	cvt.rn.f64.s32	%fd871, %r345;
	neg.f64 	%fd872, %fd871;
	fma.rn.f64 	%fd874, %fd872, %fd577, %fd1417;
	fma.rn.f64 	%fd876, %fd872, %fd579, %fd874;
	fma.rn.f64 	%fd1419, %fd872, %fd581, %fd876;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r231}, %fd1417;
	}
	and.b32  	%r232, %r231, 2145386496;
	setp.lt.u32	%p99, %r232, 1105199104;
	@%p99 bra 	BB1_110;

	// Callseq Start 10
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1417;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd57;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1419, [retval0+0];
	
	//{
	}// Callseq End 10
	ld.local.u32 	%r345, [%rd58];

BB1_110:
	add.s32 	%r72, %r345, 1;
	and.b32  	%r233, %r72, 1;
	shl.b32 	%r234, %r233, 3;
	setp.eq.s32	%p100, %r233, 0;
	selp.f64	%fd878, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p100;
	mul.wide.u32 	%rd131, %r234, 8;
	add.s64 	%rd133, %rd131, %rd62;
	ld.const.f64 	%fd879, [%rd133+8];
	mul.rn.f64 	%fd307, %fd1419, %fd1419;
	fma.rn.f64 	%fd880, %fd878, %fd307, %fd879;
	ld.const.f64 	%fd881, [%rd133+16];
	fma.rn.f64 	%fd882, %fd880, %fd307, %fd881;
	ld.const.f64 	%fd883, [%rd133+24];
	fma.rn.f64 	%fd884, %fd882, %fd307, %fd883;
	ld.const.f64 	%fd885, [%rd133+32];
	fma.rn.f64 	%fd886, %fd884, %fd307, %fd885;
	ld.const.f64 	%fd887, [%rd133+40];
	fma.rn.f64 	%fd888, %fd886, %fd307, %fd887;
	ld.const.f64 	%fd889, [%rd133+48];
	fma.rn.f64 	%fd308, %fd888, %fd307, %fd889;
	fma.rn.f64 	%fd1420, %fd308, %fd1419, %fd1419;
	@%p100 bra 	BB1_112;

	mov.f64 	%fd890, 0d3FF0000000000000;
	fma.rn.f64 	%fd1420, %fd308, %fd307, %fd890;

BB1_112:
	and.b32  	%r235, %r72, 2;
	setp.eq.s32	%p101, %r235, 0;
	@%p101 bra 	BB1_114;

	mov.f64 	%fd891, 0d0000000000000000;
	mov.f64 	%fd892, 0dBFF0000000000000;
	fma.rn.f64 	%fd1420, %fd1420, %fd892, %fd891;

BB1_114:
	fma.rn.f64 	%fd893, %fd2, %fd1420, %fd301;
	mul.f64 	%fd314, %fd4, %fd893;
	fma.rn.f64 	%fd315, %fd215, 0d3FE0000000000000, %fd53;
	selp.f64	%fd1422, 0d3FF0000000000000, %fd315, %p3;
	mov.f64 	%fd1421, %fd315;
	mov.u32 	%r369, %r5;
	@%p4 bra 	BB1_116;

BB1_115:
	mov.f64 	%fd318, %fd1421;
	and.b32  	%r236, %r369, 1;
	setp.eq.b32	%p104, %r236, 1;
	not.pred 	%p105, %p104;
	mul.f64 	%fd319, %fd318, %fd318;
	mul.f64 	%fd894, %fd1422, %fd319;
	selp.f64	%fd1422, %fd1422, %fd894, %p105;
	shr.u32 	%r369, %r369, 1;
	setp.ne.s32	%p106, %r369, 0;
	mov.f64 	%fd1421, %fd319;
	@%p106 bra 	BB1_115;

BB1_116:
	mul.f64 	%fd895, %fd1, %fd290;
	sub.f64 	%fd896, %fd315, %fd1422;
	sub.f64 	%fd322, %fd896, %fd895;
	mov.f64 	%fd1424, %fd250;
	@%p80 bra 	BB1_118;

	mov.f64 	%fd897, 0d0000000000000000;
	mul.rn.f64 	%fd1424, %fd250, %fd897;

BB1_118:
	mul.f64 	%fd898, %fd1424, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r346, %fd898;
	st.local.u32 	[%rd58], %r346;
	cvt.rn.f64.s32	%fd899, %r346;
	neg.f64 	%fd900, %fd899;
	fma.rn.f64 	%fd902, %fd900, %fd577, %fd1424;
	fma.rn.f64 	%fd904, %fd900, %fd579, %fd902;
	fma.rn.f64 	%fd1426, %fd900, %fd581, %fd904;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r237}, %fd1424;
	}
	and.b32  	%r238, %r237, 2145386496;
	setp.lt.u32	%p108, %r238, 1105199104;
	@%p108 bra 	BB1_120;

	// Callseq Start 11
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1424;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd57;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1426, [retval0+0];
	
	//{
	}// Callseq End 11
	ld.local.u32 	%r346, [%rd58];

BB1_120:
	add.s32 	%r78, %r346, 1;
	and.b32  	%r239, %r78, 1;
	shl.b32 	%r240, %r239, 3;
	setp.eq.s32	%p109, %r239, 0;
	selp.f64	%fd906, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p109;
	mul.wide.u32 	%rd138, %r240, 8;
	add.s64 	%rd140, %rd138, %rd62;
	ld.const.f64 	%fd907, [%rd140+8];
	mul.rn.f64 	%fd328, %fd1426, %fd1426;
	fma.rn.f64 	%fd908, %fd906, %fd328, %fd907;
	ld.const.f64 	%fd909, [%rd140+16];
	fma.rn.f64 	%fd910, %fd908, %fd328, %fd909;
	ld.const.f64 	%fd911, [%rd140+24];
	fma.rn.f64 	%fd912, %fd910, %fd328, %fd911;
	ld.const.f64 	%fd913, [%rd140+32];
	fma.rn.f64 	%fd914, %fd912, %fd328, %fd913;
	ld.const.f64 	%fd915, [%rd140+40];
	fma.rn.f64 	%fd916, %fd914, %fd328, %fd915;
	ld.const.f64 	%fd917, [%rd140+48];
	fma.rn.f64 	%fd329, %fd916, %fd328, %fd917;
	fma.rn.f64 	%fd1427, %fd329, %fd1426, %fd1426;
	@%p109 bra 	BB1_122;

	mov.f64 	%fd918, 0d3FF0000000000000;
	fma.rn.f64 	%fd1427, %fd329, %fd328, %fd918;

BB1_122:
	and.b32  	%r241, %r78, 2;
	setp.eq.s32	%p110, %r241, 0;
	@%p110 bra 	BB1_124;

	mov.f64 	%fd919, 0d0000000000000000;
	mov.f64 	%fd920, 0dBFF0000000000000;
	fma.rn.f64 	%fd1427, %fd1427, %fd920, %fd919;

BB1_124:
	fma.rn.f64 	%fd921, %fd2, %fd1427, %fd322;
	mul.f64 	%fd335, %fd4, %fd921;
	fma.rn.f64 	%fd336, %fd217, 0d3FE0000000000000, %fd52;
	selp.f64	%fd1429, 0d3FF0000000000000, %fd336, %p3;
	mov.f64 	%fd1428, %fd336;
	mov.u32 	%r368, %r5;
	@%p4 bra 	BB1_126;

BB1_125:
	mov.f64 	%fd339, %fd1428;
	and.b32  	%r242, %r368, 1;
	setp.eq.b32	%p113, %r242, 1;
	not.pred 	%p114, %p113;
	mul.f64 	%fd340, %fd339, %fd339;
	mul.f64 	%fd922, %fd1429, %fd340;
	selp.f64	%fd1429, %fd1429, %fd922, %p114;
	shr.u32 	%r368, %r368, 1;
	setp.ne.s32	%p115, %r368, 0;
	mov.f64 	%fd1428, %fd340;
	@%p115 bra 	BB1_125;

BB1_126:
	mul.f64 	%fd923, %fd1, %fd292;
	sub.f64 	%fd924, %fd336, %fd1429;
	sub.f64 	%fd343, %fd924, %fd923;
	mov.f64 	%fd1431, %fd273;
	@%p89 bra 	BB1_128;

	mov.f64 	%fd925, 0d0000000000000000;
	mul.rn.f64 	%fd1431, %fd273, %fd925;

BB1_128:
	mul.f64 	%fd926, %fd1431, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r347, %fd926;
	st.local.u32 	[%rd58], %r347;
	cvt.rn.f64.s32	%fd927, %r347;
	neg.f64 	%fd928, %fd927;
	fma.rn.f64 	%fd930, %fd928, %fd577, %fd1431;
	fma.rn.f64 	%fd932, %fd928, %fd579, %fd930;
	fma.rn.f64 	%fd1433, %fd928, %fd581, %fd932;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r243}, %fd1431;
	}
	and.b32  	%r244, %r243, 2145386496;
	setp.lt.u32	%p117, %r244, 1105199104;
	@%p117 bra 	BB1_130;

	// Callseq Start 12
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1431;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd57;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1433, [retval0+0];
	
	//{
	}// Callseq End 12
	ld.local.u32 	%r347, [%rd58];

BB1_130:
	add.s32 	%r84, %r347, 1;
	and.b32  	%r245, %r84, 1;
	shl.b32 	%r246, %r245, 3;
	setp.eq.s32	%p118, %r245, 0;
	selp.f64	%fd934, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p118;
	mul.wide.u32 	%rd145, %r246, 8;
	add.s64 	%rd147, %rd145, %rd62;
	ld.const.f64 	%fd935, [%rd147+8];
	mul.rn.f64 	%fd349, %fd1433, %fd1433;
	fma.rn.f64 	%fd936, %fd934, %fd349, %fd935;
	ld.const.f64 	%fd937, [%rd147+16];
	fma.rn.f64 	%fd938, %fd936, %fd349, %fd937;
	ld.const.f64 	%fd939, [%rd147+24];
	fma.rn.f64 	%fd940, %fd938, %fd349, %fd939;
	ld.const.f64 	%fd941, [%rd147+32];
	fma.rn.f64 	%fd942, %fd940, %fd349, %fd941;
	ld.const.f64 	%fd943, [%rd147+40];
	fma.rn.f64 	%fd944, %fd942, %fd349, %fd943;
	ld.const.f64 	%fd945, [%rd147+48];
	fma.rn.f64 	%fd350, %fd944, %fd349, %fd945;
	fma.rn.f64 	%fd1434, %fd350, %fd1433, %fd1433;
	@%p118 bra 	BB1_132;

	mov.f64 	%fd946, 0d3FF0000000000000;
	fma.rn.f64 	%fd1434, %fd350, %fd349, %fd946;

BB1_132:
	and.b32  	%r247, %r84, 2;
	setp.eq.s32	%p119, %r247, 0;
	@%p119 bra 	BB1_134;

	mov.f64 	%fd947, 0d0000000000000000;
	mov.f64 	%fd948, 0dBFF0000000000000;
	fma.rn.f64 	%fd1434, %fd1434, %fd948, %fd947;

BB1_134:
	fma.rn.f64 	%fd949, %fd2, %fd1434, %fd343;
	mul.f64 	%fd356, %fd4, %fd949;
	add.f64 	%fd357, %fd1290, %fd356;
	add.f64 	%fd358, %fd54, %fd289;
	selp.f64	%fd1436, 0d3FF0000000000000, %fd358, %p3;
	mov.f64 	%fd1435, %fd358;
	mov.u32 	%r367, %r5;
	@%p4 bra 	BB1_136;

BB1_135:
	mov.f64 	%fd361, %fd1435;
	and.b32  	%r248, %r367, 1;
	setp.eq.b32	%p122, %r248, 1;
	not.pred 	%p123, %p122;
	mul.f64 	%fd362, %fd361, %fd361;
	mul.f64 	%fd950, %fd1436, %fd362;
	selp.f64	%fd1436, %fd1436, %fd950, %p123;
	shr.u32 	%r367, %r367, 1;
	setp.ne.s32	%p124, %r367, 0;
	mov.f64 	%fd1435, %fd362;
	@%p124 bra 	BB1_135;

BB1_136:
	add.f64 	%fd951, %fd4, %fd1282;
	mul.f64 	%fd1437, %fd3, %fd951;
	abs.f64 	%fd952, %fd1437;
	setp.neu.f64	%p125, %fd952, 0d7FF0000000000000;
	@%p125 bra 	BB1_138;

	mov.f64 	%fd953, 0d0000000000000000;
	mul.rn.f64 	%fd1437, %fd1437, %fd953;

BB1_138:
	mul.f64 	%fd954, %fd1437, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r348, %fd954;
	st.local.u32 	[%rd58], %r348;
	cvt.rn.f64.s32	%fd955, %r348;
	neg.f64 	%fd956, %fd955;
	fma.rn.f64 	%fd958, %fd956, %fd577, %fd1437;
	fma.rn.f64 	%fd960, %fd956, %fd579, %fd958;
	fma.rn.f64 	%fd1438, %fd956, %fd581, %fd960;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r249}, %fd1437;
	}
	and.b32  	%r250, %r249, 2145386496;
	setp.lt.u32	%p126, %r250, 1105199104;
	@%p126 bra 	BB1_140;

	// Callseq Start 13
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1437;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd57;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1438, [retval0+0];
	
	//{
	}// Callseq End 13
	ld.local.u32 	%r348, [%rd58];

BB1_140:
	add.s32 	%r90, %r348, 1;
	and.b32  	%r251, %r90, 1;
	shl.b32 	%r252, %r251, 3;
	setp.eq.s32	%p127, %r251, 0;
	selp.f64	%fd962, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p127;
	mul.wide.u32 	%rd152, %r252, 8;
	add.s64 	%rd154, %rd152, %rd62;
	ld.const.f64 	%fd963, [%rd154+8];
	mul.rn.f64 	%fd371, %fd1438, %fd1438;
	fma.rn.f64 	%fd964, %fd962, %fd371, %fd963;
	ld.const.f64 	%fd965, [%rd154+16];
	fma.rn.f64 	%fd966, %fd964, %fd371, %fd965;
	ld.const.f64 	%fd967, [%rd154+24];
	fma.rn.f64 	%fd968, %fd966, %fd371, %fd967;
	ld.const.f64 	%fd969, [%rd154+32];
	fma.rn.f64 	%fd970, %fd968, %fd371, %fd969;
	ld.const.f64 	%fd971, [%rd154+40];
	fma.rn.f64 	%fd972, %fd970, %fd371, %fd971;
	ld.const.f64 	%fd973, [%rd154+48];
	fma.rn.f64 	%fd372, %fd972, %fd371, %fd973;
	fma.rn.f64 	%fd1439, %fd372, %fd1438, %fd1438;
	@%p127 bra 	BB1_142;

	mov.f64 	%fd974, 0d3FF0000000000000;
	fma.rn.f64 	%fd1439, %fd372, %fd371, %fd974;

BB1_142:
	and.b32  	%r253, %r90, 2;
	setp.eq.s32	%p128, %r253, 0;
	@%p128 bra 	BB1_144;

	mov.f64 	%fd975, 0d0000000000000000;
	mov.f64 	%fd976, 0dBFF0000000000000;
	fma.rn.f64 	%fd1439, %fd1439, %fd976, %fd975;

BB1_144:
	add.f64 	%fd378, %fd53, %fd291;
	selp.f64	%fd1441, 0d3FF0000000000000, %fd378, %p3;
	mov.f64 	%fd1440, %fd378;
	mov.u32 	%r366, %r5;
	@%p4 bra 	BB1_146;

BB1_145:
	mov.f64 	%fd381, %fd1440;
	and.b32  	%r254, %r366, 1;
	setp.eq.b32	%p131, %r254, 1;
	not.pred 	%p132, %p131;
	mul.f64 	%fd382, %fd381, %fd381;
	mul.f64 	%fd977, %fd1441, %fd382;
	selp.f64	%fd1441, %fd1441, %fd977, %p132;
	shr.u32 	%r366, %r366, 1;
	setp.ne.s32	%p133, %r366, 0;
	mov.f64 	%fd1440, %fd382;
	@%p133 bra 	BB1_145;

BB1_146:
	add.f64 	%fd978, %fd4, %fd1274;
	mul.f64 	%fd1442, %fd3, %fd978;
	abs.f64 	%fd979, %fd1442;
	setp.neu.f64	%p134, %fd979, 0d7FF0000000000000;
	@%p134 bra 	BB1_148;

	mov.f64 	%fd980, 0d0000000000000000;
	mul.rn.f64 	%fd1442, %fd1442, %fd980;

BB1_148:
	mul.f64 	%fd981, %fd1442, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r349, %fd981;
	st.local.u32 	[%rd58], %r349;
	cvt.rn.f64.s32	%fd982, %r349;
	neg.f64 	%fd983, %fd982;
	fma.rn.f64 	%fd985, %fd983, %fd577, %fd1442;
	fma.rn.f64 	%fd987, %fd983, %fd579, %fd985;
	fma.rn.f64 	%fd1443, %fd983, %fd581, %fd987;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r255}, %fd1442;
	}
	and.b32  	%r256, %r255, 2145386496;
	setp.lt.u32	%p135, %r256, 1105199104;
	@%p135 bra 	BB1_150;

	// Callseq Start 14
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1442;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd57;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1443, [retval0+0];
	
	//{
	}// Callseq End 14
	ld.local.u32 	%r349, [%rd58];

BB1_150:
	add.s32 	%r96, %r349, 1;
	and.b32  	%r257, %r96, 1;
	shl.b32 	%r258, %r257, 3;
	setp.eq.s32	%p136, %r257, 0;
	selp.f64	%fd989, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p136;
	mul.wide.u32 	%rd159, %r258, 8;
	add.s64 	%rd161, %rd159, %rd62;
	ld.const.f64 	%fd990, [%rd161+8];
	mul.rn.f64 	%fd391, %fd1443, %fd1443;
	fma.rn.f64 	%fd991, %fd989, %fd391, %fd990;
	ld.const.f64 	%fd992, [%rd161+16];
	fma.rn.f64 	%fd993, %fd991, %fd391, %fd992;
	ld.const.f64 	%fd994, [%rd161+24];
	fma.rn.f64 	%fd995, %fd993, %fd391, %fd994;
	ld.const.f64 	%fd996, [%rd161+32];
	fma.rn.f64 	%fd997, %fd995, %fd391, %fd996;
	ld.const.f64 	%fd998, [%rd161+40];
	fma.rn.f64 	%fd999, %fd997, %fd391, %fd998;
	ld.const.f64 	%fd1000, [%rd161+48];
	fma.rn.f64 	%fd392, %fd999, %fd391, %fd1000;
	fma.rn.f64 	%fd1444, %fd392, %fd1443, %fd1443;
	@%p136 bra 	BB1_152;

	mov.f64 	%fd1001, 0d3FF0000000000000;
	fma.rn.f64 	%fd1444, %fd392, %fd391, %fd1001;

BB1_152:
	and.b32  	%r259, %r96, 2;
	setp.eq.s32	%p137, %r259, 0;
	@%p137 bra 	BB1_154;

	mov.f64 	%fd1002, 0d0000000000000000;
	mov.f64 	%fd1003, 0dBFF0000000000000;
	fma.rn.f64 	%fd1444, %fd1444, %fd1003, %fd1002;

BB1_154:
	add.f64 	%fd1004, %fd1298, %fd335;
	add.f64 	%fd1005, %fd1306, %fd314;
	mul.f64 	%fd398, %fd4, %fd1005;
	mul.f64 	%fd399, %fd4, %fd1004;
	mul.f64 	%fd400, %fd4, %fd357;
	mul.f64 	%fd1006, %fd1, %fd1005;
	sub.f64 	%fd1007, %fd358, %fd1436;
	sub.f64 	%fd1008, %fd1007, %fd1006;
	fma.rn.f64 	%fd1009, %fd2, %fd1439, %fd1008;
	mul.f64 	%fd401, %fd4, %fd1009;
	mul.f64 	%fd1010, %fd1, %fd1004;
	sub.f64 	%fd1011, %fd378, %fd1441;
	sub.f64 	%fd1012, %fd1011, %fd1010;
	fma.rn.f64 	%fd402, %fd2, %fd1444, %fd1012;
	add.f64 	%fd403, %fd52, %fd293;
	selp.f64	%fd1446, 0d3FF0000000000000, %fd403, %p3;
	mov.u32 	%r365, %r5;
	mov.f64 	%fd1445, %fd403;
	@%p4 bra 	BB1_156;

BB1_155:
	mov.f64 	%fd406, %fd1445;
	and.b32  	%r260, %r365, 1;
	setp.eq.b32	%p140, %r260, 1;
	not.pred 	%p141, %p140;
	mul.f64 	%fd407, %fd406, %fd406;
	mul.f64 	%fd1013, %fd1446, %fd407;
	selp.f64	%fd1446, %fd1446, %fd1013, %p141;
	shr.u32 	%r365, %r365, 1;
	setp.ne.s32	%p142, %r365, 0;
	mov.f64 	%fd1445, %fd407;
	@%p142 bra 	BB1_155;

BB1_156:
	mul.f64 	%fd410, %fd4, %fd402;
	mul.f64 	%fd1014, %fd1, %fd357;
	sub.f64 	%fd1015, %fd403, %fd1446;
	sub.f64 	%fd411, %fd1015, %fd1014;
	add.f64 	%fd1016, %fd4, %fd1266;
	mul.f64 	%fd1447, %fd3, %fd1016;
	abs.f64 	%fd1017, %fd1447;
	setp.neu.f64	%p143, %fd1017, 0d7FF0000000000000;
	@%p143 bra 	BB1_158;

	mov.f64 	%fd1018, 0d0000000000000000;
	mul.rn.f64 	%fd1447, %fd1447, %fd1018;

BB1_158:
	mul.f64 	%fd1019, %fd1447, 0d3FE45F306DC9C883;
	cvt.rni.s32.f64	%r381, %fd1019;
	st.local.u32 	[%rd58], %r381;
	cvt.rn.f64.s32	%fd1020, %r381;
	neg.f64 	%fd1021, %fd1020;
	fma.rn.f64 	%fd1023, %fd1021, %fd577, %fd1447;
	fma.rn.f64 	%fd1025, %fd1021, %fd579, %fd1023;
	fma.rn.f64 	%fd1448, %fd1021, %fd581, %fd1025;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r261}, %fd1447;
	}
	and.b32  	%r262, %r261, 2145386496;
	setp.lt.u32	%p144, %r262, 1105199104;
	@%p144 bra 	BB1_160;

	// Callseq Start 15
	{
	.reg .b32 temp_param_reg;
	// <end>}
	.param .b64 param0;
	st.param.f64	[param0+0], %fd1447;
	.param .b64 param1;
	st.param.b64	[param1+0], %rd57;
	.param .b64 retval0;
	call.uni (retval0), 
	__internal_trig_reduction_slowpathd, 
	(
	param0, 
	param1
	);
	ld.param.f64	%fd1448, [retval0+0];
	
	//{
	}// Callseq End 15
	ld.local.u32 	%r381, [%rd58];

BB1_160:
	add.s32 	%r102, %r381, 1;
	and.b32  	%r263, %r102, 1;
	shl.b32 	%r264, %r263, 3;
	setp.eq.s32	%p145, %r263, 0;
	selp.f64	%fd1027, 0d3DE5DB65F9785EBA, 0dBDA8FF8320FD8164, %p145;
	mul.wide.u32 	%rd166, %r264, 8;
	add.s64 	%rd168, %rd166, %rd62;
	ld.const.f64 	%fd1028, [%rd168+8];
	mul.rn.f64 	%fd418, %fd1448, %fd1448;
	fma.rn.f64 	%fd1029, %fd1027, %fd418, %fd1028;
	ld.const.f64 	%fd1030, [%rd168+16];
	fma.rn.f64 	%fd1031, %fd1029, %fd418, %fd1030;
	ld.const.f64 	%fd1032, [%rd168+24];
	fma.rn.f64 	%fd1033, %fd1031, %fd418, %fd1032;
	ld.const.f64 	%fd1034, [%rd168+32];
	fma.rn.f64 	%fd1035, %fd1033, %fd418, %fd1034;
	ld.const.f64 	%fd1036, [%rd168+40];
	fma.rn.f64 	%fd1037, %fd1035, %fd418, %fd1036;
	ld.const.f64 	%fd1038, [%rd168+48];
	fma.rn.f64 	%fd419, %fd1037, %fd418, %fd1038;
	fma.rn.f64 	%fd1449, %fd419, %fd1448, %fd1448;
	@%p145 bra 	BB1_162;

	mov.f64 	%fd1039, 0d3FF0000000000000;
	fma.rn.f64 	%fd1449, %fd419, %fd418, %fd1039;

BB1_162:
	and.b32  	%r265, %r102, 2;
	setp.eq.s32	%p146, %r265, 0;
	@%p146 bra 	BB1_164;

	mov.f64 	%fd1040, 0d0000000000000000;
	mov.f64 	%fd1041, 0dBFF0000000000000;
	fma.rn.f64 	%fd1449, %fd1449, %fd1041, %fd1040;

BB1_164:
	fma.rn.f64 	%fd1042, %fd2, %fd1449, %fd411;
	fma.rn.f64 	%fd1043, %fd80, 0d4000000000000000, %fd58;
	fma.rn.f64 	%fd1044, %fd105, 0d4000000000000000, %fd1043;
	add.f64 	%fd1045, %fd1044, %fd147;
	fma.rn.f64 	%fd1046, %fd103, 0d4000000000000000, %fd78;
	fma.rn.f64 	%fd1047, %fd126, 0d4000000000000000, %fd1046;
	add.f64 	%fd1048, %fd1047, %fd225;
	fma.rn.f64 	%fd1049, %fd213, 0d4000000000000000, %fd149;
	fma.rn.f64 	%fd1050, %fd289, 0d4000000000000000, %fd1049;
	add.f64 	%fd1051, %fd1050, %fd398;
	fma.rn.f64 	%fd1052, %fd215, 0d4000000000000000, %fd150;
	fma.rn.f64 	%fd1053, %fd291, 0d4000000000000000, %fd1052;
	add.f64 	%fd1054, %fd1053, %fd399;
	fma.rn.f64 	%fd1055, %fd217, 0d4000000000000000, %fd151;
	fma.rn.f64 	%fd1056, %fd293, 0d4000000000000000, %fd1055;
	add.f64 	%fd1057, %fd1056, %fd400;
	fma.rn.f64 	%fd1058, %fd241, 0d4000000000000000, %fd171;
	fma.rn.f64 	%fd1059, %fd314, 0d4000000000000000, %fd1058;
	add.f64 	%fd1060, %fd1059, %fd401;
	fma.rn.f64 	%fd1061, %fd264, 0d4000000000000000, %fd191;
	fma.rn.f64 	%fd1062, %fd335, 0d4000000000000000, %fd1061;
	add.f64 	%fd1063, %fd1062, %fd410;
	fma.rn.f64 	%fd1064, %fd287, 0d4000000000000000, %fd211;
	fma.rn.f64 	%fd1065, %fd356, 0d4000000000000000, %fd1064;
	fma.rn.f64 	%fd1066, %fd4, %fd1042, %fd1065;
	fma.rn.f64 	%fd1366, %fd1045, 0d3FC5555555555555, %fd57;
	fma.rn.f64 	%fd1355, %fd1048, 0d3FC5555555555555, %fd1355;
	add.f64 	%fd1346, %fd18, %fd1346;
	fma.rn.f64 	%fd1336, %fd1051, 0d3FC5555555555555, %fd54;
	fma.rn.f64 	%fd1326, %fd1054, 0d3FC5555555555555, %fd53;
	fma.rn.f64 	%fd1316, %fd1057, 0d3FC5555555555555, %fd52;
	fma.rn.f64 	%fd1306, %fd1060, 0d3FC5555555555555, %fd1306;
	fma.rn.f64 	%fd1298, %fd1063, 0d3FC5555555555555, %fd1298;
	fma.rn.f64 	%fd1290, %fd1066, 0d3FC5555555555555, %fd1290;
	add.f64 	%fd1282, %fd18, %fd1282;
	add.f64 	%fd1274, %fd18, %fd1274;
	add.f64 	%fd1266, %fd18, %fd1266;
	add.s32 	%r334, %r334, 1;
	setp.lt.u32	%p147, %r334, %r1;
	mov.f64 	%fd1265, %fd1266;
	mov.f64 	%fd1273, %fd1274;
	mov.f64 	%fd1281, %fd1282;
	mov.f64 	%fd1289, %fd1290;
	mov.f64 	%fd1297, %fd1298;
	mov.f64 	%fd1305, %fd1306;
	mov.f64 	%fd1314, %fd1316;
	mov.f64 	%fd1324, %fd1326;
	mov.f64 	%fd1334, %fd1336;
	mov.f64 	%fd1347, %fd1346;
	mov.f64 	%fd1356, %fd1355;
	mov.f64 	%fd1367, %fd1366;
	@%p147 bra 	BB1_4;

BB1_165:
	mov.f64 	%fd1369, %fd1367;
	mov.f64 	%fd1358, %fd1356;
	mov.f64 	%fd1349, %fd1347;
	mov.f64 	%fd1333, %fd1334;
	mov.f64 	%fd1323, %fd1324;
	mov.f64 	%fd1313, %fd1314;
	mov.f64 	%fd1304, %fd1305;
	mov.f64 	%fd1296, %fd1297;
	mov.f64 	%fd1288, %fd1289;
	mov.f64 	%fd1280, %fd1281;
	mov.f64 	%fd1272, %fd1273;
	mov.f64 	%fd1264, %fd1265;
	mov.u32 	%r319, 2;
	abs.s32 	%r318, %r319;
	shr.u32 	%r104, %r318, 1;
	setp.eq.s32	%p148, %r104, 0;
	and.b32  	%r105, %r318, 1;
	setp.eq.s32	%p149, %r105, 0;
	sub.f64 	%fd449, %fd1358, %fd1304;
	sub.f64 	%fd450, %fd1349, %fd1280;
	sub.f64 	%fd451, %fd1369, %fd1333;
	selp.f64	%fd1451, 0d3FF0000000000000, %fd451, %p149;
	mov.f64 	%fd1450, %fd451;
	mov.u32 	%r398, %r104;
	@%p148 bra 	BB1_167;

BB1_166:
	mov.u32 	%r106, %r398;
	mov.f64 	%fd454, %fd1450;
	and.b32  	%r266, %r106, 1;
	setp.eq.b32	%p150, %r266, 1;
	not.pred 	%p151, %p150;
	mul.f64 	%fd455, %fd454, %fd454;
	mul.f64 	%fd1067, %fd1451, %fd455;
	selp.f64	%fd1451, %fd1451, %fd1067, %p151;
	shr.u32 	%r107, %r106, 1;
	setp.ne.s32	%p152, %r107, 0;
	mov.f64 	%fd1450, %fd455;
	mov.u32 	%r398, %r107;
	@%p152 bra 	BB1_166;

BB1_167:
	selp.f64	%fd1453, 0d3FF0000000000000, %fd449, %p149;
	mov.f64 	%fd1452, %fd449;
	mov.u32 	%r397, %r104;
	@%p148 bra 	BB1_169;

BB1_168:
	mov.f64 	%fd460, %fd1452;
	and.b32  	%r267, %r397, 1;
	setp.eq.b32	%p155, %r267, 1;
	not.pred 	%p156, %p155;
	mul.f64 	%fd461, %fd460, %fd460;
	mul.f64 	%fd1068, %fd1453, %fd461;
	selp.f64	%fd1453, %fd1453, %fd1068, %p156;
	shr.u32 	%r397, %r397, 1;
	setp.ne.s32	%p157, %r397, 0;
	mov.f64 	%fd1452, %fd461;
	@%p157 bra 	BB1_168;

BB1_169:
	selp.f64	%fd1455, 0d3FF0000000000000, %fd450, %p149;
	mov.f64 	%fd1454, %fd450;
	mov.u32 	%r396, %r104;
	@%p148 bra 	BB1_171;

BB1_170:
	mov.f64 	%fd466, %fd1454;
	and.b32  	%r268, %r396, 1;
	setp.eq.b32	%p160, %r268, 1;
	not.pred 	%p161, %p160;
	mul.f64 	%fd467, %fd466, %fd466;
	mul.f64 	%fd1069, %fd1455, %fd467;
	selp.f64	%fd1455, %fd1455, %fd1069, %p161;
	shr.u32 	%r396, %r396, 1;
	setp.ne.s32	%p162, %r396, 0;
	mov.f64 	%fd1454, %fd467;
	@%p162 bra 	BB1_170;

BB1_171:
	sub.f64 	%fd1070, %fd1369, %fd1323;
	sub.f64 	%fd1071, %fd1358, %fd1296;
	sub.f64 	%fd1072, %fd1349, %fd1272;
	add.f64 	%fd1073, %fd1451, %fd1453;
	add.f64 	%fd1074, %fd1073, %fd1455;
	sqrt.rn.f64 	%fd470, %fd1074;
	div.rn.f64 	%fd471, %fd451, %fd470;
	div.rn.f64 	%fd472, %fd449, %fd470;
	div.rn.f64 	%fd473, %fd450, %fd470;
	mul.f64 	%fd1075, %fd1071, %fd472;
	fma.rn.f64 	%fd1076, %fd1070, %fd471, %fd1075;
	fma.rn.f64 	%fd1077, %fd1072, %fd473, %fd1076;
	mul.f64 	%fd1078, %fd472, %fd472;
	fma.rn.f64 	%fd1079, %fd471, %fd471, %fd1078;
	fma.rn.f64 	%fd474, %fd473, %fd473, %fd1079;
	div.rn.f64 	%fd1080, %fd1077, %fd474;
	mul.f64 	%fd1081, %fd471, %fd1080;
	sub.f64 	%fd475, %fd1070, %fd1081;
	mul.f64 	%fd1082, %fd472, %fd1080;
	sub.f64 	%fd476, %fd1071, %fd1082;
	mul.f64 	%fd1083, %fd473, %fd1080;
	sub.f64 	%fd477, %fd1072, %fd1083;
	selp.f64	%fd1457, 0d3FF0000000000000, %fd475, %p149;
	mov.f64 	%fd1456, %fd475;
	mov.u32 	%r395, %r104;
	@%p148 bra 	BB1_173;

BB1_172:
	mov.f64 	%fd480, %fd1456;
	and.b32  	%r269, %r395, 1;
	setp.eq.b32	%p165, %r269, 1;
	not.pred 	%p166, %p165;
	mul.f64 	%fd481, %fd480, %fd480;
	mul.f64 	%fd1084, %fd1457, %fd481;
	selp.f64	%fd1457, %fd1457, %fd1084, %p166;
	shr.u32 	%r395, %r395, 1;
	setp.ne.s32	%p167, %r395, 0;
	mov.f64 	%fd1456, %fd481;
	@%p167 bra 	BB1_172;

BB1_173:
	selp.f64	%fd1459, 0d3FF0000000000000, %fd476, %p149;
	mov.f64 	%fd1458, %fd476;
	mov.u32 	%r394, %r104;
	@%p148 bra 	BB1_175;

BB1_174:
	mov.f64 	%fd486, %fd1458;
	and.b32  	%r270, %r394, 1;
	setp.eq.b32	%p170, %r270, 1;
	not.pred 	%p171, %p170;
	mul.f64 	%fd487, %fd486, %fd486;
	mul.f64 	%fd1085, %fd1459, %fd487;
	selp.f64	%fd1459, %fd1459, %fd1085, %p171;
	shr.u32 	%r394, %r394, 1;
	setp.ne.s32	%p172, %r394, 0;
	mov.f64 	%fd1458, %fd487;
	@%p172 bra 	BB1_174;

BB1_175:
	selp.f64	%fd1461, 0d3FF0000000000000, %fd477, %p149;
	mov.f64 	%fd1460, %fd477;
	mov.u32 	%r393, %r104;
	@%p148 bra 	BB1_177;

BB1_176:
	mov.f64 	%fd492, %fd1460;
	and.b32  	%r271, %r393, 1;
	setp.eq.b32	%p175, %r271, 1;
	not.pred 	%p176, %p175;
	mul.f64 	%fd493, %fd492, %fd492;
	mul.f64 	%fd1086, %fd1461, %fd493;
	selp.f64	%fd1461, %fd1461, %fd1086, %p176;
	shr.u32 	%r393, %r393, 1;
	setp.ne.s32	%p177, %r393, 0;
	mov.f64 	%fd1460, %fd493;
	@%p177 bra 	BB1_176;

BB1_177:
	sub.f64 	%fd1087, %fd1369, %fd1313;
	sub.f64 	%fd1088, %fd1358, %fd1288;
	sub.f64 	%fd1089, %fd1349, %fd1264;
	add.f64 	%fd1090, %fd1457, %fd1459;
	add.f64 	%fd1091, %fd1090, %fd1461;
	sqrt.rn.f64 	%fd496, %fd1091;
	div.rn.f64 	%fd497, %fd475, %fd496;
	div.rn.f64 	%fd498, %fd476, %fd496;
	div.rn.f64 	%fd499, %fd477, %fd496;
	mul.f64 	%fd1092, %fd1088, %fd472;
	fma.rn.f64 	%fd1093, %fd1087, %fd471, %fd1092;
	fma.rn.f64 	%fd1094, %fd1089, %fd473, %fd1093;
	div.rn.f64 	%fd1095, %fd1094, %fd474;
	mul.f64 	%fd1096, %fd1088, %fd498;
	fma.rn.f64 	%fd1097, %fd1087, %fd497, %fd1096;
	fma.rn.f64 	%fd1098, %fd1089, %fd499, %fd1097;
	mul.f64 	%fd1099, %fd498, %fd498;
	fma.rn.f64 	%fd1100, %fd497, %fd497, %fd1099;
	fma.rn.f64 	%fd1101, %fd499, %fd499, %fd1100;
	div.rn.f64 	%fd1102, %fd1098, %fd1101;
	mul.f64 	%fd1103, %fd471, %fd1095;
	sub.f64 	%fd1104, %fd1087, %fd1103;
	mul.f64 	%fd1105, %fd497, %fd1102;
	sub.f64 	%fd500, %fd1104, %fd1105;
	mul.f64 	%fd1106, %fd472, %fd1095;
	sub.f64 	%fd1107, %fd1088, %fd1106;
	mul.f64 	%fd1108, %fd498, %fd1102;
	sub.f64 	%fd501, %fd1107, %fd1108;
	mul.f64 	%fd1109, %fd473, %fd1095;
	sub.f64 	%fd1110, %fd1089, %fd1109;
	mul.f64 	%fd1111, %fd499, %fd1102;
	sub.f64 	%fd502, %fd1110, %fd1111;
	selp.f64	%fd1463, 0d3FF0000000000000, %fd500, %p149;
	mov.f64 	%fd1462, %fd500;
	mov.u32 	%r392, %r104;
	@%p148 bra 	BB1_179;

BB1_178:
	mov.f64 	%fd505, %fd1462;
	and.b32  	%r272, %r392, 1;
	setp.eq.b32	%p180, %r272, 1;
	not.pred 	%p181, %p180;
	mul.f64 	%fd506, %fd505, %fd505;
	mul.f64 	%fd1112, %fd1463, %fd506;
	selp.f64	%fd1463, %fd1463, %fd1112, %p181;
	shr.u32 	%r392, %r392, 1;
	setp.ne.s32	%p182, %r392, 0;
	mov.f64 	%fd1462, %fd506;
	@%p182 bra 	BB1_178;

BB1_179:
	selp.f64	%fd1465, 0d3FF0000000000000, %fd501, %p149;
	mov.f64 	%fd1464, %fd501;
	mov.u32 	%r391, %r104;
	@%p148 bra 	BB1_181;

BB1_180:
	mov.f64 	%fd511, %fd1464;
	and.b32  	%r273, %r391, 1;
	setp.eq.b32	%p185, %r273, 1;
	not.pred 	%p186, %p185;
	mul.f64 	%fd512, %fd511, %fd511;
	mul.f64 	%fd1113, %fd1465, %fd512;
	selp.f64	%fd1465, %fd1465, %fd1113, %p186;
	shr.u32 	%r391, %r391, 1;
	setp.ne.s32	%p187, %r391, 0;
	mov.f64 	%fd1464, %fd512;
	@%p187 bra 	BB1_180;

BB1_181:
	add.f64 	%fd515, %fd1463, %fd1465;
	selp.f64	%fd1467, 0d3FF0000000000000, %fd502, %p149;
	mov.u32 	%r390, %r104;
	mov.f64 	%fd1466, %fd502;
	@%p148 bra 	BB1_183;

BB1_182:
	mov.f64 	%fd518, %fd1466;
	and.b32  	%r274, %r390, 1;
	setp.eq.b32	%p190, %r274, 1;
	not.pred 	%p191, %p190;
	mul.f64 	%fd519, %fd518, %fd518;
	mul.f64 	%fd1114, %fd1467, %fd519;
	selp.f64	%fd1467, %fd1467, %fd1114, %p191;
	shr.u32 	%r390, %r390, 1;
	setp.ne.s32	%p192, %r390, 0;
	mov.f64 	%fd1466, %fd519;
	@%p192 bra 	BB1_182;

BB1_183:
	add.f64 	%fd1115, %fd515, %fd1467;
	sqrt.rn.f64 	%fd522, %fd1115;
	div.rn.f64 	%fd523, %fd470, 0d3F1A36E2EB1C432D;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r399}, %fd523;
	}
	setp.gt.f64	%p193, %fd523, 0d0000000000000000;
	setp.lt.s32	%p194, %r399, 2146435072;
	and.pred  	%p195, %p193, %p194;
	@%p195 bra 	BB1_188;
	bra.uni 	BB1_184;

BB1_188:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r400, %temp}, %fd523;
	}
	mov.u32 	%r401, -1023;
	setp.gt.s32	%p199, %r399, 1048575;
	@%p199 bra 	BB1_190;

	mul.f64 	%fd1118, %fd523, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r399}, %fd1118;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r400, %temp}, %fd1118;
	}
	mov.u32 	%r401, -1077;

BB1_190:
	shr.u32 	%r277, %r399, 20;
	add.s32 	%r402, %r401, %r277;
	and.b32  	%r278, %r399, -2146435073;
	or.b32  	%r279, %r278, 1072693248;
	mov.b64 	%fd1468, {%r400, %r279};
	setp.lt.s32	%p200, %r279, 1073127583;
	@%p200 bra 	BB1_192;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r280, %temp}, %fd1468;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r281}, %fd1468;
	}
	add.s32 	%r282, %r281, -1048576;
	mov.b64 	%fd1468, {%r280, %r282};
	add.s32 	%r402, %r402, 1;

BB1_192:
	add.f64 	%fd1120, %fd1468, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd1119,%fd1120;
	// inline asm
	neg.f64 	%fd1121, %fd1120;
	mov.f64 	%fd1122, 0d3FF0000000000000;
	fma.rn.f64 	%fd1123, %fd1121, %fd1119, %fd1122;
	fma.rn.f64 	%fd1124, %fd1123, %fd1123, %fd1123;
	fma.rn.f64 	%fd1125, %fd1124, %fd1119, %fd1119;
	add.f64 	%fd1126, %fd1468, 0dBFF0000000000000;
	mul.f64 	%fd1127, %fd1126, %fd1125;
	fma.rn.f64 	%fd1128, %fd1126, %fd1125, %fd1127;
	mul.f64 	%fd1129, %fd1128, %fd1128;
	mov.f64 	%fd1130, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd1131, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd1132, %fd1131, %fd1129, %fd1130;
	mov.f64 	%fd1133, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd1134, %fd1132, %fd1129, %fd1133;
	mov.f64 	%fd1135, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd1136, %fd1134, %fd1129, %fd1135;
	mov.f64 	%fd1137, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd1138, %fd1136, %fd1129, %fd1137;
	mov.f64 	%fd1139, 0d3F624924923BE72D;
	fma.rn.f64 	%fd1140, %fd1138, %fd1129, %fd1139;
	mov.f64 	%fd1141, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd1142, %fd1140, %fd1129, %fd1141;
	mov.f64 	%fd1143, 0d3FB5555555555554;
	fma.rn.f64 	%fd1144, %fd1142, %fd1129, %fd1143;
	sub.f64 	%fd1145, %fd1126, %fd1128;
	add.f64 	%fd1146, %fd1145, %fd1145;
	neg.f64 	%fd1147, %fd1128;
	fma.rn.f64 	%fd1148, %fd1147, %fd1126, %fd1146;
	mul.f64 	%fd1149, %fd1125, %fd1148;
	mul.f64 	%fd1150, %fd1129, %fd1144;
	fma.rn.f64 	%fd1151, %fd1150, %fd1128, %fd1149;
	xor.b32  	%r283, %r402, -2147483648;
	mov.u32 	%r284, 1127219200;
	mov.b64 	%fd1152, {%r283, %r284};
	mov.u32 	%r285, -2147483648;
	mov.b64 	%fd1153, {%r285, %r284};
	sub.f64 	%fd1154, %fd1152, %fd1153;
	mov.f64 	%fd1155, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd1156, %fd1154, %fd1155, %fd1128;
	neg.f64 	%fd1157, %fd1154;
	fma.rn.f64 	%fd1158, %fd1157, %fd1155, %fd1156;
	sub.f64 	%fd1159, %fd1158, %fd1128;
	sub.f64 	%fd1160, %fd1151, %fd1159;
	mov.f64 	%fd1161, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd1162, %fd1154, %fd1161, %fd1160;
	add.f64 	%fd1469, %fd1156, %fd1162;
	bra.uni 	BB1_193;

BB1_184:
	abs.f64 	%fd1116, %fd523;
	setp.gtu.f64	%p196, %fd1116, 0d7FF0000000000000;
	@%p196 bra 	BB1_187;
	bra.uni 	BB1_185;

BB1_187:
	add.f64 	%fd1469, %fd523, %fd523;
	bra.uni 	BB1_193;

BB1_185:
	setp.eq.f64	%p197, %fd523, 0d0000000000000000;
	mov.f64 	%fd1469, 0dFFF0000000000000;
	@%p197 bra 	BB1_193;

	setp.eq.f64	%p198, %fd523, 0d7FF0000000000000;
	selp.f64	%fd1469, %fd523, 0dFFF8000000000000, %p198;

BB1_193:
	add.f64 	%fd1340, %fd1340, %fd1469;
	div.rn.f64 	%fd532, %fd496, 0d3F1A36E2EB1C432D;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r403}, %fd532;
	}
	setp.gt.f64	%p201, %fd532, 0d0000000000000000;
	setp.lt.s32	%p202, %r403, 2146435072;
	and.pred  	%p203, %p201, %p202;
	@%p203 bra 	BB1_198;
	bra.uni 	BB1_194;

BB1_198:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r404, %temp}, %fd532;
	}
	mov.u32 	%r405, -1023;
	setp.gt.s32	%p207, %r403, 1048575;
	@%p207 bra 	BB1_200;

	mul.f64 	%fd1165, %fd532, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r403}, %fd1165;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r404, %temp}, %fd1165;
	}
	mov.u32 	%r405, -1077;

BB1_200:
	shr.u32 	%r288, %r403, 20;
	add.s32 	%r406, %r405, %r288;
	and.b32  	%r289, %r403, -2146435073;
	or.b32  	%r290, %r289, 1072693248;
	mov.b64 	%fd1470, {%r404, %r290};
	setp.lt.s32	%p208, %r290, 1073127583;
	@%p208 bra 	BB1_202;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r291, %temp}, %fd1470;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r292}, %fd1470;
	}
	add.s32 	%r293, %r292, -1048576;
	mov.b64 	%fd1470, {%r291, %r293};
	add.s32 	%r406, %r406, 1;

BB1_202:
	add.f64 	%fd1167, %fd1470, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd1166,%fd1167;
	// inline asm
	neg.f64 	%fd1168, %fd1167;
	mov.f64 	%fd1169, 0d3FF0000000000000;
	fma.rn.f64 	%fd1170, %fd1168, %fd1166, %fd1169;
	fma.rn.f64 	%fd1171, %fd1170, %fd1170, %fd1170;
	fma.rn.f64 	%fd1172, %fd1171, %fd1166, %fd1166;
	add.f64 	%fd1173, %fd1470, 0dBFF0000000000000;
	mul.f64 	%fd1174, %fd1173, %fd1172;
	fma.rn.f64 	%fd1175, %fd1173, %fd1172, %fd1174;
	mul.f64 	%fd1176, %fd1175, %fd1175;
	mov.f64 	%fd1177, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd1178, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd1179, %fd1178, %fd1176, %fd1177;
	mov.f64 	%fd1180, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd1181, %fd1179, %fd1176, %fd1180;
	mov.f64 	%fd1182, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd1183, %fd1181, %fd1176, %fd1182;
	mov.f64 	%fd1184, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd1185, %fd1183, %fd1176, %fd1184;
	mov.f64 	%fd1186, 0d3F624924923BE72D;
	fma.rn.f64 	%fd1187, %fd1185, %fd1176, %fd1186;
	mov.f64 	%fd1188, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd1189, %fd1187, %fd1176, %fd1188;
	mov.f64 	%fd1190, 0d3FB5555555555554;
	fma.rn.f64 	%fd1191, %fd1189, %fd1176, %fd1190;
	sub.f64 	%fd1192, %fd1173, %fd1175;
	add.f64 	%fd1193, %fd1192, %fd1192;
	neg.f64 	%fd1194, %fd1175;
	fma.rn.f64 	%fd1195, %fd1194, %fd1173, %fd1193;
	mul.f64 	%fd1196, %fd1172, %fd1195;
	mul.f64 	%fd1197, %fd1176, %fd1191;
	fma.rn.f64 	%fd1198, %fd1197, %fd1175, %fd1196;
	xor.b32  	%r294, %r406, -2147483648;
	mov.u32 	%r295, 1127219200;
	mov.b64 	%fd1199, {%r294, %r295};
	mov.u32 	%r296, -2147483648;
	mov.b64 	%fd1200, {%r296, %r295};
	sub.f64 	%fd1201, %fd1199, %fd1200;
	mov.f64 	%fd1202, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd1203, %fd1201, %fd1202, %fd1175;
	neg.f64 	%fd1204, %fd1201;
	fma.rn.f64 	%fd1205, %fd1204, %fd1202, %fd1203;
	sub.f64 	%fd1206, %fd1205, %fd1175;
	sub.f64 	%fd1207, %fd1198, %fd1206;
	mov.f64 	%fd1208, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd1209, %fd1201, %fd1208, %fd1207;
	add.f64 	%fd1471, %fd1203, %fd1209;
	bra.uni 	BB1_203;

BB1_194:
	abs.f64 	%fd1163, %fd532;
	setp.gtu.f64	%p204, %fd1163, 0d7FF0000000000000;
	@%p204 bra 	BB1_197;
	bra.uni 	BB1_195;

BB1_197:
	add.f64 	%fd1471, %fd532, %fd532;
	bra.uni 	BB1_203;

BB1_195:
	setp.eq.f64	%p205, %fd532, 0d0000000000000000;
	mov.f64 	%fd1471, 0dFFF0000000000000;
	@%p205 bra 	BB1_203;

	setp.eq.f64	%p206, %fd532, 0d7FF0000000000000;
	selp.f64	%fd1471, %fd532, 0dFFF8000000000000, %p206;

BB1_203:
	div.rn.f64 	%fd540, %fd500, %fd522;
	div.rn.f64 	%fd541, %fd501, %fd522;
	div.rn.f64 	%fd542, %fd502, %fd522;
	add.f64 	%fd1339, %fd1339, %fd1471;
	div.rn.f64 	%fd544, %fd522, 0d3F1A36E2EB1C432D;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r407}, %fd544;
	}
	setp.gt.f64	%p209, %fd544, 0d0000000000000000;
	setp.lt.s32	%p210, %r407, 2146435072;
	and.pred  	%p211, %p209, %p210;
	@%p211 bra 	BB1_208;
	bra.uni 	BB1_204;

BB1_208:
	{
	.reg .b32 %temp; 
	mov.b64 	{%r408, %temp}, %fd544;
	}
	mov.u32 	%r409, -1023;
	setp.gt.s32	%p215, %r407, 1048575;
	@%p215 bra 	BB1_210;

	mul.f64 	%fd1212, %fd544, 0d4350000000000000;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r407}, %fd1212;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%r408, %temp}, %fd1212;
	}
	mov.u32 	%r409, -1077;

BB1_210:
	shr.u32 	%r299, %r407, 20;
	add.s32 	%r410, %r409, %r299;
	and.b32  	%r300, %r407, -2146435073;
	or.b32  	%r301, %r300, 1072693248;
	mov.b64 	%fd1472, {%r408, %r301};
	setp.lt.s32	%p216, %r301, 1073127583;
	@%p216 bra 	BB1_212;

	{
	.reg .b32 %temp; 
	mov.b64 	{%r302, %temp}, %fd1472;
	}
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r303}, %fd1472;
	}
	add.s32 	%r304, %r303, -1048576;
	mov.b64 	%fd1472, {%r302, %r304};
	add.s32 	%r410, %r410, 1;

BB1_212:
	add.f64 	%fd1214, %fd1472, 0d3FF0000000000000;
	// inline asm
	rcp.approx.ftz.f64 %fd1213,%fd1214;
	// inline asm
	neg.f64 	%fd1215, %fd1214;
	mov.f64 	%fd1216, 0d3FF0000000000000;
	fma.rn.f64 	%fd1217, %fd1215, %fd1213, %fd1216;
	fma.rn.f64 	%fd1218, %fd1217, %fd1217, %fd1217;
	fma.rn.f64 	%fd1219, %fd1218, %fd1213, %fd1213;
	add.f64 	%fd1220, %fd1472, 0dBFF0000000000000;
	mul.f64 	%fd1221, %fd1220, %fd1219;
	fma.rn.f64 	%fd1222, %fd1220, %fd1219, %fd1221;
	mul.f64 	%fd1223, %fd1222, %fd1222;
	mov.f64 	%fd1224, 0d3ED0EE258B7A8B04;
	mov.f64 	%fd1225, 0d3EB1380B3AE80F1E;
	fma.rn.f64 	%fd1226, %fd1225, %fd1223, %fd1224;
	mov.f64 	%fd1227, 0d3EF3B2669F02676F;
	fma.rn.f64 	%fd1228, %fd1226, %fd1223, %fd1227;
	mov.f64 	%fd1229, 0d3F1745CBA9AB0956;
	fma.rn.f64 	%fd1230, %fd1228, %fd1223, %fd1229;
	mov.f64 	%fd1231, 0d3F3C71C72D1B5154;
	fma.rn.f64 	%fd1232, %fd1230, %fd1223, %fd1231;
	mov.f64 	%fd1233, 0d3F624924923BE72D;
	fma.rn.f64 	%fd1234, %fd1232, %fd1223, %fd1233;
	mov.f64 	%fd1235, 0d3F8999999999A3C4;
	fma.rn.f64 	%fd1236, %fd1234, %fd1223, %fd1235;
	mov.f64 	%fd1237, 0d3FB5555555555554;
	fma.rn.f64 	%fd1238, %fd1236, %fd1223, %fd1237;
	sub.f64 	%fd1239, %fd1220, %fd1222;
	add.f64 	%fd1240, %fd1239, %fd1239;
	neg.f64 	%fd1241, %fd1222;
	fma.rn.f64 	%fd1242, %fd1241, %fd1220, %fd1240;
	mul.f64 	%fd1243, %fd1219, %fd1242;
	mul.f64 	%fd1244, %fd1223, %fd1238;
	fma.rn.f64 	%fd1245, %fd1244, %fd1222, %fd1243;
	xor.b32  	%r305, %r410, -2147483648;
	mov.u32 	%r306, 1127219200;
	mov.b64 	%fd1246, {%r305, %r306};
	mov.u32 	%r307, -2147483648;
	mov.b64 	%fd1247, {%r307, %r306};
	sub.f64 	%fd1248, %fd1246, %fd1247;
	mov.f64 	%fd1249, 0d3FE62E42FEFA39EF;
	fma.rn.f64 	%fd1250, %fd1248, %fd1249, %fd1222;
	neg.f64 	%fd1251, %fd1248;
	fma.rn.f64 	%fd1252, %fd1251, %fd1249, %fd1250;
	sub.f64 	%fd1253, %fd1252, %fd1222;
	sub.f64 	%fd1254, %fd1245, %fd1253;
	mov.f64 	%fd1255, 0d3C7ABC9E3B39803F;
	fma.rn.f64 	%fd1256, %fd1248, %fd1255, %fd1254;
	add.f64 	%fd1473, %fd1250, %fd1256;
	bra.uni 	BB1_213;

BB1_204:
	abs.f64 	%fd1210, %fd544;
	setp.gtu.f64	%p212, %fd1210, 0d7FF0000000000000;
	@%p212 bra 	BB1_207;
	bra.uni 	BB1_205;

BB1_207:
	add.f64 	%fd1473, %fd544, %fd544;
	bra.uni 	BB1_213;

BB1_205:
	setp.eq.f64	%p213, %fd544, 0d0000000000000000;
	mov.f64 	%fd1473, 0dFFF0000000000000;
	@%p213 bra 	BB1_213;

	setp.eq.f64	%p214, %fd544, 0d7FF0000000000000;
	selp.f64	%fd1473, %fd544, 0dFFF8000000000000, %p214;

BB1_213:
	add.f64 	%fd1338, %fd1338, %fd1473;
	fma.rn.f64 	%fd1337, %fd471, 0d3F1A36E2EB1C432D, %fd1369;
	fma.rn.f64 	%fd1327, %fd497, 0d3F1A36E2EB1C432D, %fd1369;
	fma.rn.f64 	%fd1317, %fd540, 0d3F1A36E2EB1C432D, %fd1369;
	fma.rn.f64 	%fd1307, %fd472, 0d3F1A36E2EB1C432D, %fd1358;
	fma.rn.f64 	%fd1299, %fd498, 0d3F1A36E2EB1C432D, %fd1358;
	fma.rn.f64 	%fd1291, %fd541, 0d3F1A36E2EB1C432D, %fd1358;
	fma.rn.f64 	%fd1283, %fd473, 0d3F1A36E2EB1C432D, %fd1349;
	fma.rn.f64 	%fd1275, %fd499, 0d3F1A36E2EB1C432D, %fd1349;
	fma.rn.f64 	%fd1267, %fd542, 0d3F1A36E2EB1C432D, %fd1349;
	add.s32 	%r333, %r333, 1;
	setp.lt.u32	%p217, %r333, 20000;
	mov.f64 	%fd1348, %fd1349;
	mov.f64 	%fd1357, %fd1358;
	mov.f64 	%fd1368, %fd1369;
	@%p217 bra 	BB1_2;

	mov.u32 	%r329, %ntid.x;
	ld.param.u64 	%rd185, [_Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_2];
	ld.param.u64 	%rd184, [_Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_1];
	mov.u32 	%r328, %ntid.y;
	mov.u32 	%r327, %ctaid.x;
	mov.u32 	%r326, %ctaid.y;
	mov.u32 	%r325, %nctaid.x;
	mad.lo.s32 	%r324, %r325, %r326, %r327;
	mul.lo.s32 	%r323, %r328, %r329;
	mov.u32 	%r322, %tid.x;
	mov.u32 	%r321, %tid.y;
	mad.lo.s32 	%r320, %r321, %r329, %r322;
	cvt.u64.u32	%rd183, %r320;
	mul.wide.u32 	%rd182, %r323, %r324;
	add.s64 	%rd181, %rd182, %rd183;
	shl.b64 	%rd180, %rd181, 3;
	ld.param.u64 	%rd179, [_Z28processDuffing1989RK4_BifurcPdS_S_PKdS1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_S1_j_param_0];
	div.rn.f64 	%fd1257, %fd1340, 0d40C3880000000000;
	cvta.to.global.u64 	%rd172, %rd179;
	add.s64 	%rd174, %rd172, %rd180;
	st.global.f64 	[%rd174], %fd1257;
	cvta.to.global.u64 	%rd175, %rd184;
	add.s64 	%rd176, %rd175, %rd180;
	div.rn.f64 	%fd1258, %fd1339, 0d40C3880000000000;
	st.global.f64 	[%rd176], %fd1258;
	cvta.to.global.u64 	%rd177, %rd185;
	add.s64 	%rd178, %rd177, %rd180;
	div.rn.f64 	%fd1259, %fd1338, 0d40C3880000000000;
	st.global.f64 	[%rd178], %fd1259;

BB1_215:
	ret;
}

.func  (.param .b64 func_retval0) __internal_trig_reduction_slowpathd(
	.param .b64 __internal_trig_reduction_slowpathd_param_0,
	.param .b64 __internal_trig_reduction_slowpathd_param_1
)
{
	.local .align 8 .b8 	__local_depot2[40];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<9>;
	.reg .s32 	%r<43>;
	.reg .f64 	%fd<5>;
	.reg .s64 	%rd<100>;


	mov.u64 	%rd99, __local_depot2;
	cvta.local.u64 	%SP, %rd99;
	ld.param.f64 	%fd4, [__internal_trig_reduction_slowpathd_param_0];
	ld.param.u64 	%rd36, [__internal_trig_reduction_slowpathd_param_1];
	add.u64 	%rd37, %SP, 0;
	cvta.to.local.u64 	%rd1, %rd37;
	{
	.reg .b32 %temp; 
	mov.b64 	{%temp, %r1}, %fd4;
	}
	and.b32  	%r41, %r1, -2147483648;
	shr.u32 	%r3, %r1, 20;
	bfe.u32 	%r4, %r1, 20, 11;
	setp.eq.s32	%p1, %r4, 2047;
	@%p1 bra 	BB2_13;

	add.s32 	%r16, %r4, -1024;
	shr.u32 	%r17, %r16, 6;
	mov.u32 	%r18, 16;
	sub.s32 	%r5, %r18, %r17;
	mov.u32 	%r19, 19;
	sub.s32 	%r20, %r19, %r17;
	mov.u32 	%r21, 18;
	min.s32 	%r6, %r21, %r20;
	setp.gt.s32	%p2, %r5, %r6;
	mov.u64 	%rd93, 0;
	mov.u64 	%rd91, %rd1;
	@%p2 bra 	BB2_4;

	mov.b64 	 %rd40, %fd4;
	shl.b64 	%rd41, %rd40, 11;
	or.b64  	%rd3, %rd41, -9223372036854775808;
	add.s32 	%r7, %r5, -1;
	bfe.u32 	%r22, %r1, 20, 11;
	add.s32 	%r23, %r22, -1024;
	shr.u32 	%r24, %r23, 6;
	mov.u32 	%r25, 15;
	sub.s32 	%r26, %r25, %r24;
	mul.wide.s32 	%rd42, %r26, 8;
	mov.u64 	%rd43, __cudart_i2opi_d;
	add.s64 	%rd88, %rd43, %rd42;
	mov.u64 	%rd93, 0;
	mov.u64 	%rd89, %rd1;
	mov.u32 	%r40, %r7;
	mov.u64 	%rd92, %rd91;

BB2_3:
	.pragma "nounroll";
	mov.u32 	%r8, %r40;
	mov.u64 	%rd6, %rd89;
	ld.const.u64 	%rd46, [%rd88];
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi, clo, chi;
	mov.b64         {alo,ahi}, %rd46;    
	mov.b64         {blo,bhi}, %rd3;    
	mov.b64         {clo,chi}, %rd93;    
	mad.lo.cc.u32   r0, alo, blo, clo;
	madc.hi.cc.u32  r1, alo, blo, chi;
	madc.hi.u32     r2, alo, bhi,   0;
	mad.lo.cc.u32   r1, alo, bhi,  r1;
	madc.hi.cc.u32  r2, ahi, blo,  r2;
	madc.hi.u32     r3, ahi, bhi,   0;
	mad.lo.cc.u32   r1, ahi, blo,  r1;
	madc.lo.cc.u32  r2, ahi, bhi,  r2;
	addc.u32        r3,  r3,   0;     
	mov.b64         %rd44, {r0,r1};      
	mov.b64         %rd45, {r2,r3};      
	}
	// inline asm
	mov.u64 	%rd93, %rd45;
	st.local.u64 	[%rd92], %rd44;
	add.s32 	%r9, %r8, 1;
	sub.s32 	%r27, %r9, %r7;
	mul.wide.s32 	%rd49, %r27, 8;
	add.s64 	%rd92, %rd1, %rd49;
	add.s64 	%rd12, %rd6, 8;
	mov.u64 	%rd91, %rd12;
	add.s64 	%rd88, %rd88, 8;
	setp.lt.s32	%p3, %r9, %r6;
	mov.u64 	%rd89, %rd12;
	mov.u32 	%r40, %r9;
	@%p3 bra 	BB2_3;

BB2_4:
	st.local.u64 	[%rd91], %rd93;
	ld.local.u64 	%rd94, [%rd1+16];
	ld.local.u64 	%rd95, [%rd1+24];
	and.b32  	%r10, %r3, 63;
	setp.eq.s32	%p4, %r10, 0;
	@%p4 bra 	BB2_6;

	mov.u32 	%r28, 64;
	sub.s32 	%r29, %r28, %r10;
	shl.b64 	%rd50, %rd95, %r10;
	shr.u64 	%rd51, %rd94, %r29;
	or.b64  	%rd95, %rd50, %rd51;
	shl.b64 	%rd52, %rd94, %r10;
	ld.local.u64 	%rd53, [%rd1+8];
	shr.u64 	%rd54, %rd53, %r29;
	or.b64  	%rd94, %rd54, %rd52;

BB2_6:
	cvta.to.local.u64 	%rd55, %rd36;
	shr.u64 	%rd56, %rd95, 62;
	cvt.u32.u64	%r30, %rd56;
	shr.u64 	%rd57, %rd94, 62;
	shl.b64 	%rd58, %rd95, 2;
	or.b64  	%rd97, %rd58, %rd57;
	shl.b64 	%rd96, %rd94, 2;
	shr.u64 	%rd59, %rd95, 61;
	cvt.u32.u64	%r31, %rd59;
	and.b32  	%r32, %r31, 1;
	add.s32 	%r33, %r32, %r30;
	neg.s32 	%r34, %r33;
	setp.eq.s32	%p5, %r41, 0;
	selp.b32	%r35, %r33, %r34, %p5;
	st.local.u32 	[%rd55], %r35;
	setp.eq.s32	%p6, %r32, 0;
	@%p6 bra 	BB2_8;

	mov.u64 	%rd63, 0;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd63;
	mov.b64         {a2,a3}, %rd63;
	mov.b64         {b0,b1}, %rd96;
	mov.b64         {b2,b3}, %rd97;
	sub.cc.u32      r0, a0, b0; 
	subc.cc.u32     r1, a1, b1; 
	subc.cc.u32     r2, a2, b2; 
	subc.u32        r3, a3, b3; 
	mov.b64         %rd60, {r0,r1};
	mov.b64         %rd61, {r2,r3};
	}
	// inline asm
	mov.u64 	%rd96, %rd60;
	mov.u64 	%rd97, %rd61;
	xor.b32  	%r41, %r41, -2147483648;

BB2_8:
	clz.b64 	%r42, %rd97;
	setp.eq.s32	%p7, %r42, 0;
	@%p7 bra 	BB2_10;

	shl.b64 	%rd66, %rd97, %r42;
	mov.u32 	%r36, 64;
	sub.s32 	%r37, %r36, %r42;
	shr.u64 	%rd67, %rd96, %r37;
	or.b64  	%rd97, %rd67, %rd66;

BB2_10:
	mov.u64 	%rd71, -3958705157555305931;
	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, alo, ahi, blo, bhi;
	mov.b64         {alo,ahi}, %rd97;   
	mov.b64         {blo,bhi}, %rd71;   
	mul.lo.u32      r0, alo, blo;    
	mul.hi.u32      r1, alo, blo;    
	mad.lo.cc.u32   r1, alo, bhi, r1;
	madc.hi.u32     r2, alo, bhi,  0;
	mad.lo.cc.u32   r1, ahi, blo, r1;
	madc.hi.cc.u32  r2, ahi, blo, r2;
	madc.hi.u32     r3, ahi, bhi,  0;
	mad.lo.cc.u32   r2, ahi, bhi, r2;
	addc.u32        r3, r3,  0;      
	mov.b64         %rd68, {r0,r1};     
	mov.b64         %rd69, {r2,r3};     
	}
	// inline asm
	mov.u64 	%rd98, %rd69;
	setp.lt.s64	%p8, %rd69, 1;
	@%p8 bra 	BB2_12;

	// inline asm
	{
	.reg .u32 r0, r1, r2, r3, a0, a1, a2, a3, b0, b1, b2, b3;
	mov.b64         {a0,a1}, %rd68;
	mov.b64         {a2,a3}, %rd69;
	mov.b64         {b0,b1}, %rd68;
	mov.b64         {b2,b3}, %rd69;
	add.cc.u32      r0, a0, b0; 
	addc.cc.u32     r1, a1, b1; 
	addc.cc.u32     r2, a2, b2; 
	addc.u32        r3, a3, b3; 
	mov.b64         %rd72, {r0,r1};
	mov.b64         %rd73, {r2,r3};
	}
	// inline asm
	mov.u64 	%rd98, %rd73;
	add.s32 	%r42, %r42, 1;

BB2_12:
	cvt.u64.u32	%rd78, %r41;
	shl.b64 	%rd79, %rd78, 32;
	mov.u32 	%r38, 1022;
	sub.s32 	%r39, %r38, %r42;
	cvt.u64.u32	%rd80, %r39;
	shl.b64 	%rd81, %rd80, 52;
	add.s64 	%rd82, %rd98, 1;
	shr.u64 	%rd83, %rd82, 10;
	add.s64 	%rd84, %rd83, 1;
	shr.u64 	%rd85, %rd84, 1;
	add.s64 	%rd86, %rd85, %rd81;
	or.b64  	%rd87, %rd86, %rd79;
	mov.b64 	 %fd4, %rd87;

BB2_13:
	st.param.f64	[func_retval0+0], %fd4;
	ret;
}


